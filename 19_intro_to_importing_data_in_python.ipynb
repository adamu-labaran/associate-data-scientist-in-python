{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Introduction and flat files\n",
    "\n",
    ">In this chapter, you'll learn how to import data into Python from all types of flat files, which are a simple and prevalent form of data storage. You've previously learned how to use NumPy and pandas—you will learn how to use these packages to import flat files and customize your imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Importing entire text files\n",
    "\n",
    ">In this exercise, you'll be working with the file moby_dick.txt. \n",
    "\n",
    ">It is a text file that contains the opening sentences of Moby Dick, one of the great American novels! In the video, you've seen you could open such a file using __file = open('moby_dick.txt', mode='r')__. \n",
    "\n",
    ">You could then read from it with file.read() and close the file using file.close(). \n",
    "\n",
    ">However, using context managers allows you to do this more effectively. \n",
    "\n",
    ">In this exercise, you will gain experience opening a text file, printing its contents and, finally, closing it by using a context manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Open the file moby_dick.txt as read-only using a with statement and bind it to the variable file. \n",
    "\n",
    ">Make sure to pass the filename enclosed in quotation marks ''.\n",
    "\n",
    ">Print the contents of the file to the shell using the print() function. \n",
    "\n",
    ">As Hugo showed in the video, you'll need to apply the method .read() to the object file and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER 1. Loomings.\n",
      "    \n",
      "    Call me Ishmael. Some years ago--never mind how long precisely--having\n",
      "    little or no money in my purse, and nothing particular to interest me on\n",
      "    shore, I thought I would sail about a little and see the watery part of\n",
      "    the world. It is a way I have of driving off the spleen and regulating\n",
      "    the circulation. Whenever I find myself growing grim about the mouth;\n",
      "    whenever it is a damp, drizzly November in my soul; whenever I find\n",
      "    myself involuntarily pausing before coffin warehouses, and bringing up\n",
      "    the rear of every funeral I meet; and especially whenever my hypos get\n",
      "    such an upper hand of me, that it requires a strong moral principle to\n",
      "    prevent me from deliberately stepping into the street, and methodically\n",
      "    knocking people's hats off--then, I account it high time to get to sea\n",
      "    as soon as I can. This is my substitute for pistol and ball. With a\n",
      "    philosophical flourish Cato throws himself upon his sword; I quietly\n",
      "    take to the ship. There is nothing surprising in this. If they but knew\n",
      "    it, almost all men in their degree, some time or other, cherish very\n",
      "    nearly the same feelings towards the ocean with me.\n",
      "# Open a file as read-only and bind it to file\n",
      "with open('moby_dick.txt', 'r') as file:\n",
      "    # Print it\n",
      "    print(file.read())\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago--never mind how long precisely--having\n",
      "little or no money in my purse, and nothing particular to interest me on\n",
      "shore, I thought I would sail about a little and see the watery part of\n",
      "the world. It is a way I have of driving off the spleen and regulating\n",
      "the circulation. Whenever I find myself growing grim about the mouth;\n",
      "whenever it is a damp, drizzly November in my soul; whenever I find\n",
      "myself involuntarily pausing before coffin warehouses, and bringing up\n",
      "the rear of every funeral I meet; and especially whenever my hypos get\n",
      "such an upper hand of me, that it requires a strong moral principle to\n",
      "prevent me from deliberately stepping into the street, and methodically\n",
      "knocking people's hats off--then, I account it high time to get to sea\n",
      "as soon as I can. This is my substitute for pistol and ball. With a\n",
      "philosophical flourish Cato throws himself upon his sword; I quietly\n",
      "take to the ship. There is nothing surprising in this. If they but knew\n",
      "it, almost all men in their degree, some time or other, cherish very\n",
      "nearly the same feelings towards the ocean with me.\n"
     ]
    }
   ],
   "source": [
    "with open('moby_dick.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER 1. Loomings.\n",
      "    \n",
      "    Call me Ishmael. Some years ago--never mind how long precisely--having\n",
      "    little or no money in my purse, and nothing particular to interest me on\n",
      "    shore, I thought I would sail about a little and see the watery part of\n",
      "    the world. It is a way I have of driving off the spleen and regulating\n",
      "    the circulation. Whenever I find myself growing grim about the mouth;\n",
      "    whenever it is a damp, drizzly November in my soul; whenever I find\n",
      "    myself involuntarily pausing before coffin warehouses, and bringing up\n",
      "    the rear of every funeral I meet; and especially whenever my hypos get\n",
      "    such an upper hand of me, that it requires a strong moral principle to\n",
      "    prevent me from deliberately stepping into the street, and methodically\n",
      "    knocking people's hats off--then, I account it high time to get to sea\n",
      "    as soon as I can. This is my substitute for pistol and ball. With a\n",
      "    philosophical flourish Cato throws himself upon his sword; I quietly\n",
      "    take to the ship. There is nothing surprising in this. If they but knew\n",
      "    it, almost all men in their degree, some time or other, cherish very\n",
      "    nearly the same feelings towards the ocean with me.\n",
      "# Open a file as read-only and bind it to file\n",
      "with open('moby_dick.txt', 'r') as file:\n",
      "    # Print it\n",
      "    print(file.read())\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "Call me Ishmael. Some years ago--never mind how long precisely--having\n",
      "little or no money in my purse, and nothing particular to interest me on\n",
      "shore, I thought I would sail about a little and see the watery part of\n",
      "the world. It is a way I have of driving off the spleen and regulating\n",
      "the circulation. Whenever I find myself growing grim about the mouth;\n",
      "whenever it is a damp, drizzly November in my soul; whenever I find\n",
      "myself involuntarily pausing before coffin warehouses, and bringing up\n",
      "the rear of every funeral I meet; and especially whenever my hypos get\n",
      "such an upper hand of me, that it requires a strong moral principle to\n",
      "prevent me from deliberately stepping into the street, and methodically\n",
      "knocking people's hats off--then, I account it high time to get to sea\n",
      "as soon as I can. This is my substitute for pistol and ball. With a\n",
      "philosophical flourish Cato throws himself upon his sword; I quietly\n",
      "take to the ship. There is nothing surprising in this. If they but knew\n",
      "it, almost all men in their degree, some time or other, cherish very\n",
      "nearly the same feelings towards the ocean with me.\n"
     ]
    }
   ],
   "source": [
    "# Open a file as read-only and bind it to file\n",
    "with open('moby_dick.txt', 'r') as file:\n",
    "    # Print it\n",
    "    print(file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Importing text files line by line\n",
    "\n",
    ">For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines.\n",
    "\n",
    "> Enter the .readline() method, which allows you to do this. When a file called file is open, you can print out the first line by executing file.readline(). \n",
    "\n",
    ">If you execute the same command again, the second line will print, and so on.\n",
    "\n",
    ">In the introductory video, Hugo also introduced the concept of a context manager. He showed that you can bind a variable file by using a context manager construct:\n",
    "\n",
    ">with open('huck_finn.txt') as file:\n",
    "\n",
    ">While still within this construct, the variable file will be bound to open('huck_finn.txt'); thus, to print the file to the shell, all the code you need to execute is:\n",
    "\n",
    ">with open('huck_finn.txt') as file:\n",
    "    print(file.readline())\n",
    "\n",
    ">You'll now use these tools to print the first few lines of moby_dick.txt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Open moby_dick.txt using the with context manager and the variable file.\n",
    "\n",
    ">Print the first three lines of the file to the shell by using .readline() three times within the context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [2. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [5. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Using np.loadtxt()\n",
    "import numpy as np\n",
    "\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load the entire CSV file\n",
    "digits = np.loadtxt(file, delimiter=',')\n",
    "\n",
    "# Print the entire content\n",
    "print(digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            print(df)\n",
      "0       1.0  0.0  0.0.1  0.0.2  0.0.3  ...  0.0.68...\n",
      "1   0   0.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "2   1   1.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "3   2   4.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "4   3   0.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "5   4   0.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "6   ..  ...  ...    ...    ...    ...  ...      .....\n",
      "7   94  9.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "8   95  1.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "9   96  2.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "10  97  0.0  0.0    0.0    0.0    0.0  ...      0....\n",
      "11  98  5.0  0.0    0.0    0.0    0.0  ...      0....\n"
     ]
    }
   ],
   "source": [
    "# Using pandas for Better Formatting\n",
    "import pandas as pd\n",
    "\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Print the entire DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1.0  0.0  0.0.1  0.0.2  0.0.3  0.0.4  0.0.5  0.0.6  0.0.7  0.0.8  ...  \\\n",
      "0   0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "1   1.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "2   4.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "3   0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "4   0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "..  ...  ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
      "94  9.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "95  1.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "96  2.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "97  0.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "98  5.0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
      "\n",
      "    0.0.677  0.0.678  0.0.679  0.0.680  0.0.681  0.0.682  0.0.683  0.0.684  \\\n",
      "0       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "1       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "3       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "4       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "..      ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "94      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "95      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "96      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "97      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "98      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "    0.0.685  0.0.686  \n",
      "0       0.0      0.0  \n",
      "1       0.0      0.0  \n",
      "2       0.0      0.0  \n",
      "3       0.0      0.0  \n",
      "4       0.0      0.0  \n",
      "..      ...      ...  \n",
      "94      0.0      0.0  \n",
      "95      0.0      0.0  \n",
      "96      0.0      0.0  \n",
      "97      0.0      0.0  \n",
      "98      0.0      0.0  \n",
      "\n",
      "[99 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = 'digits.csv'\n",
    "df = pd.read_csv(file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Customizing your NumPy import\n",
    "\n",
    ">What if there are rows, such as a header, that you don't want to import? What if your file has a delimiter other than a comma? What if you only wish to import particular columns?\n",
    "\n",
    ">There are a number of arguments that np.loadtxt() takes that you'll find useful:\n",
    "\n",
    ">delimiter changes the delimiter that loadtxt() is expecting.\n",
    "\n",
    ">You can use ',' for comma-delimited.\n",
    "\n",
    ">You can use '\\t' for tab-delimited.\n",
    "\n",
    ">skiprows allows you to specify how many rows (not indices) you wish to skip.\n",
    "\n",
    ">usecols takes a list of the indices of the columns you wish to keep.\n",
    "\n",
    ">The file that you'll be importing, digits_header.txt, has a header and is tab-delimited.\n",
    "\n",
    ">Instructions\n",
    "\n",
    ">Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.\n",
    "\n",
    ">Complete the argument of the print() call in order to print the entire array that you just imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [3. 0.]\n",
      " [5. 0.]\n",
      " [3. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [8. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [7. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [4. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [7. 0.]\n",
      " [7. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [1. 0.]\n",
      " [9. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [0. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [5. 0.]]\n",
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [3. 0.]\n",
      " [5. 0.]\n",
      " [3. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [8. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [7. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [4. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [7. 0.]\n",
      " [7. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [1. 0.]\n",
      " [9. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [0. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0,2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data (tab-delimited, skip first row, select first and third columns)\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [3. 0.]\n",
      " [5. 0.]\n",
      " [3. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [8. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [7. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [4. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [7. 0.]\n",
      " [7. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [1. 0.]\n",
      " [9. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [0. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data (tab-delimited, skip first row, select first and third columns)\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [3. 0.]\n",
      " [5. 0.]\n",
      " [3. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [8. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [7. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [1. 0.]\n",
      " [3. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [4. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [7. 0.]\n",
      " [7. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [5. 0.]\n",
      " [1. 0.]\n",
      " [9. 0.]\n",
      " [0. 0.]\n",
      " [2. 0.]\n",
      " [2. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [0. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [3. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [7. 0.]\n",
      " [1. 0.]\n",
      " [4. 0.]\n",
      " [1. 0.]\n",
      " [5. 0.]\n",
      " [4. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [9. 0.]\n",
      " [8. 0.]\n",
      " [9. 0.]\n",
      " [6. 0.]\n",
      " [3. 0.]\n",
      " [6. 0.]\n",
      " [4. 0.]\n",
      " [6. 0.]\n",
      " [2. 0.]\n",
      " [9. 0.]\n",
      " [1. 0.]\n",
      " [2. 0.]\n",
      " [0. 0.]\n",
      " [5. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data (tab-delimited, skip first row, select first and third columns)\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first elemet: ['Time' 'Percent']\n",
      "the tenth element: [0.    0.357]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file as string data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(f'The first elemet: {data[0]}')\n",
    "\n",
    "# Import file as float and skip the first row\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(f'the tenth element: {data_float[9]}')  # Indexing starts at 0, so the 10th element is at index 9\n",
    "\n",
    "# import file as string data\n",
    "data = np.loadtxt(file, delimeter='\\t', dtype=str)\n",
    "\n",
    "# display all the contents of data\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time' 'Percent']\n",
      "[0.    0.357]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file as string data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import file as float and skip the first row\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])  # Indexing starts at 0, so the 10th element is at index 9\n",
    "\n",
    "# Plot scatter plot\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\tPercent\n",
      "99\t0.067\n",
      "99\t0.133\n",
      "99\t0.067\n",
      "99\t0\n",
      "99\t0\n",
      "0\t0.5\n",
      "0\t0.467\n",
      "0\t0.857\n",
      "0\t0.5\n",
      "0\t0.357\n",
      "0\t0.533\n",
      "5\t0.467\n",
      "5\t0.467\n",
      "5\t0.125\n",
      "5\t0.4\n",
      "5\t0.214\n",
      "5\t0.4\n",
      "10\t0.067\n",
      "10\t0.067\n",
      "10\t0.333\n",
      "10\t0.333\n",
      "10\t0.133\n",
      "10\t0.133\n",
      "15\t0.267\n",
      "15\t0.286\n",
      "15\t0.333\n",
      "15\t0.214\n",
      "15\t0\n",
      "15\t0\n",
      "20\t0.267\n",
      "20\t0.2\n",
      "20\t0.267\n",
      "20\t0.437\n",
      "20\t0.077\n",
      "20\t0.067\n",
      "25\t0.133\n",
      "25\t0.267\n",
      "25\t0.412\n",
      "25\t0\n",
      "25\t0.067\n",
      "25\t0.133\n",
      "30\t0\n",
      "30\t0.071\n",
      "30\t0\n",
      "30\t0.067\n",
      "30\t0.067\n",
      "30\t0.133\n"
     ]
    }
   ],
   "source": [
    "# Open and read the entire file\n",
    "with open('seaslug.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    \n",
    "# Print the raw contents\n",
    "print(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas to import flat files as DataFrames (1)\n",
    "In the last exercise, you were able to import flat files containing columns with different datatypes as numpy arrays. However, the DataFrame object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files of mixed data types as DataFrames using the pandas functions read_csv() and read_table().\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pandas package using the alias pd.\n",
    "Read titanic.csv into a DataFrame called df. The file name is already stored in the file object.\n",
    "In a print() call, view the head of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the entire file\n",
    "with open('titanic.csv', 'r') as file:\n",
    "    contents = file.read()\n",
    "    \n",
    "# Print the raw contents\n",
    "print(contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas to import flat files as DataFrames (2)\n",
    "In the last exercise, you were able to import flat files into a pandas DataFrame. As a bonus, it is then straightforward to retrieve the corresponding numpy array using the method .to_numpy(). You'll now have a chance to do this using the MNIST dataset, which is available as digits.csv.\n",
    "\n",
    "There are a number of arguments that pd.read_csv() takes that you'll find useful for this exercise:\n",
    "\n",
    "nrows allows you to specify how many rows to read from the file. For example, nrows=10 will only import the first 10 rows.\n",
    "header accepts row numbers to use as the column labels and marks the start of the data. If the file does not contain a header row, you can set header=None, and pandas will automatically assign integer column labels starting from 0 (e.g., 0, 1, 2, …).\n",
    "Instructions\n",
    "100 XP\n",
    "Import the first 5 rows of the file into a DataFrame using the function pd.read_csv() and assign the result to data. You'll need to use the arguments nrows and header. Note that there is no header row in this file.\n",
    "Build a numpy array from the resulting DataFrame in data and assign to data_array.\n",
    "Execute print(type(data_array)) to print the datatype of data_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = ____(____, ____, ____)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = ____\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame\n",
    "data = pd.read_csv(file, nrows=5, header=None)\n",
    "\n",
    "# Build a numpy array from the DataFrame\n",
    "data_array = data.to_numpy()\n",
    "\n",
    "# Print the datatype of data_array\n",
    "print(type(data_array))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing your pandas import\n",
    "The pandas package is great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values (NA or NaN). To wrap up this chapter, you're going to import a corrupted copy of the Titanic dataset titanic_corrupt.txt, which contains comments after the character '#', and is tab-delimited.\n",
    "\n",
    "Key arguments for pd.read_csv() include:\n",
    "\n",
    "sep sets the expected delimiter.\n",
    "You can use ',' for comma-delimited.\n",
    "You can use '\\t' for tab-delimited.\n",
    "comment takes characters that comments occur after in the file, indicating that any text starting with these characters should be ignored.\n",
    "na_values takes a list of strings to identify as NA/NaN. By default, some values are already recognized as NA/NaN. Providing this argument will supply additional values.\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the arguments of pd.read_csv() to import titanic_corrupt.txt correctly using pandas:\n",
    "sep sets the delimiter to use, and works the same way as np.loadtxt()'s delimiter argument. Note that the file you're importing is tab-delimited.\n",
    "comment takes characters that comments occur after in the file, which in this case is '#'.\n",
    "na_values takes a list of strings to be treated as NA/NaN, in this case the string 'Nothing'.\n",
    "Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the 'Age' of passengers aboard the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t', comment='#', na_values=['Nothing'])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mporting data from other file types\n",
    "\n",
    "You've learned how to import flat files, but there are many other file types you will potentially have to work with as a data scientist. In this chapter, you'll learn how to import data into Python from a wide array of important file types. These include pickled files, Excel spreadsheets, SAS and Stata files, HDF5 files, a file type for storing large quantities of numerical data, and MATLAB files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so flat any more\n",
    "You can explore your current working directory natively in Python using the library os, which consists of miscellaneous operating system interfaces.\n",
    "\n",
    "The first line of the following code imports the library os, the second line stores the name of the current directory in a string called wd and the third outputs the contents of the directory in a list to the shell.\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)\n",
    "Run this code in the shell and answer the following questions. Ignore the files that begin with ..\n",
    "\n",
    "Check out the contents of your current directory and answer the following questions: (1) which file is in your directory and NOT an example of a flat file; (2) why is it not a flat file?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "database.db is not a flat file because relational databases contain structured relationships and flat files do not.\n",
    "\n",
    "ans: battledeath.xlsx is not a flat because it is a spreadsheet consisting of many sheets, not a single table.\n",
    "\n",
    "titanic.txt is not a flat file because it is a .txt, not a .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a pickled file\n",
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n",
    "\n",
    "In this exercise, you'll import the pickle package, open a previously pickled data structure from a file and load it.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pickle package.\n",
    "Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'.\n",
    "Pass the correct argument to pickle.load(); it should use the variable that is bound to open.\n",
    "Print the data, d.\n",
    "Print the datatype of d; take your mind back to your previous use of the function type()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pickle package\n",
    "\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', ____) as file:\n",
    "    d = pickle.load(____)\n",
    "\n",
    "# Print d\n",
    "print(____)\n",
    "\n",
    "# Print datatype of d\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:  # 'rb' means read binary\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing sheets from Excel files\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Load the sheet '2004' into the DataFrame df1 using its name as a string.\n",
    "Print the head of df1 to the shell.\n",
    "Load the sheet 2002 into the DataFrame df2 using its index (0).\n",
    "Print the head of df2 to the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse(____)\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(____)\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing sheets from Excel files\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Load the sheet '2004' into the DataFrame df1 using its name as a string.\n",
    "Print the head of df1 to the shell.\n",
    "Load the sheet 2002 into the DataFrame df2 using its index (0).\n",
    "Print the head of df2 to the shell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse(____)\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(____)\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the entire Excel file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbattledeath.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m wb \u001b[38;5;241m=\u001b[39m \u001b[43mload_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Loop through all sheets and print content\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sheet_name \u001b[38;5;129;01min\u001b[39;00m wb\u001b[38;5;241m.\u001b[39msheetnames:\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\openpyxl\\reader\\excel.py:346\u001b[0m, in \u001b[0;36mload_workbook\u001b[1;34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_workbook\u001b[39m(filename, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_vba\u001b[38;5;241m=\u001b[39mKEEP_VBA,\n\u001b[0;32m    317\u001b[0m                   data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_links\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rich_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Open the given filename and return the workbook\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    :param filename: the path to open or a file-like object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mExcelReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrich_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     reader\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mwb\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\openpyxl\\reader\\excel.py:123\u001b[0m, in \u001b[0;36mExcelReader.__init__\u001b[1;34m(self, fn, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_vba\u001b[38;5;241m=\u001b[39mKEEP_VBA,\n\u001b[0;32m    122\u001b[0m              data_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_links\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rich_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchive \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchive\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_only \u001b[38;5;241m=\u001b[39m read_only\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\openpyxl\\reader\\excel.py:95\u001b[0m, in \u001b[0;36m_validate_archive\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     88\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenpyxl does not support \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m file format, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease check you can open \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit with Excel first. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupported formats are: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m%\u001b[39m (file_format,\n\u001b[0;32m     92\u001b[0m                                                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(SUPPORTED_FORMATS))\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidFileException(msg)\n\u001b[1;32m---> 95\u001b[0m archive \u001b[38;5;241m=\u001b[39m \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m archive\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\zipfile\\__init__.py:1349\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1349\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1352\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\zipfile\\__init__.py:1416\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the entire Excel file\n",
    "file = \"battledeath.xlsx\"\n",
    "wb = load_workbook(file)\n",
    "\n",
    "# Loop through all sheets and print content\n",
    "for sheet_name in wb.sheetnames:\n",
    "    sheet = wb[sheet_name]\n",
    "    print(f\"\\nSheet: {sheet_name}\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Loop through each row and print values\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        print(row)  # Print as a tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import sas7bdat package\n",
    "from ____ import ____\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    ____\n",
    "\n",
    "# Print head of DataFrame\n",
    "\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Importing SAS files\n",
    "In this exercise, you'll figure out how to import a SAS file as a DataFrame using SAS7BDAT and pandas. The file 'sales.sas7bdat' is already in your working directory and both pandas and matplotlib.pyplot have already been imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "The data are adapted from the website of the undergraduate text book Principles of Econometrics by Hill, Griffiths and Lim.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the module SAS7BDAT from the library sas7bdat.\n",
    "In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method .to_data_frame() on the object file.\n",
    "Print the head of the DataFrame df_sas.\n",
    "Execute your entire script to produce a histogram plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Stata files\n",
    "Here, you'll gain expertise in importing Stata files as DataFrames using the pd.read_stata() function from pandas. The last exercise's file, 'disarea.dta', is still in your working directory.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use pd.read_stata() to load the file 'disarea.dta' into the DataFrame df.\n",
    "Print the head of the DataFrame df.\n",
    "Visualize your results by plotting a histogram of the column disa10. We’ve already provided this code for you, so just run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using h5py to import HDF5 files\n",
    "The file 'LIGO_data.hdf5' is already in your working directory. In this exercise, you'll import it using the h5py library. You'll also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see precisely what HDF groups it contains.\n",
    "\n",
    "You can find the LIGO data plus loads of documentation and tutorials here. There is also a great tutorial on Signal Processing with the data here.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package h5py.\n",
    "Assign the name of the file to the variable file.\n",
    "Load the file as read only into the variable data.\n",
    "Print the datatype of data.\n",
    "Print the names of the groups in the HDF5 file 'LIGO_data.hdf5'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import ____\n",
    "\n",
    "# Assign filename: file\n",
    "\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(____, ____)\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in ____:\n",
    "    print(____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load file as read-only\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))  # Should output: <class 'h5py._hl.files.File'>\n",
    "\n",
    "# Print the keys (top-level groups) of the file\n",
    "print(\"Keys in the HDF5 file:\")\n",
    "for key in data.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data from your HDF5 file\n",
    "In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.\n",
    "\n",
    "To do so, you'll need to first explore the HDF5 group 'strain'.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Assign the HDF5 group data['strain'] to group.\n",
    "In the for loop, print out the keys of the HDF5 group in group.\n",
    "Assign the time series data data['strain']['Strain'] to a NumPy array called strain.\n",
    "Set num_samples equal to 10000, the number of time points we wish to sample.\n",
    "Execute the rest of the code to produce a plot of the time series data in LIGO_data.hdf5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the HDF5 group: group\n",
    "\n",
    "\n",
    "# Check out keys of group\n",
    "for key in ____:\n",
    "    print(____)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load HDF5 file as read-only\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Get the HDF5 group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of the group\n",
    "print(\"Keys in 'strain' group:\")\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data\n",
    "strain = np.array(group['Strain'])  # Convert to NumPy array\n",
    "\n",
    "# Set number of time points to sample\n",
    "num_samples = 10000  \n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('Strain')\n",
    "plt.title('LIGO Strain Data')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading .mat files\n",
    "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package scipy.io.\n",
    "Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().\n",
    "Use the function type() to print the datatype of mat to the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading .mat files\n",
    "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package scipy.io.\n",
    "Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().\n",
    "Use the function type() to print the datatype of mat to the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the scipy.io package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype of mat\n",
    "print(type(mat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "print(____)\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]  # Extract a specific row and column range\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pprint  # Pretty-print for better readability\n",
    "\n",
    "# Print all content in a readable format\n",
    "pprint.pprint(mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you'll learn how to extract meaningful data from relational databases, an essential skill for any data scientist. You will learn about relational models, how to create SQL queries, how to filter and order your SQL records, and how to perform advanced queries by joining database tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "matrix = [[i + 1 for i in range(3)] for _ in range(3)]\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop quiz: The relational model\n",
    "Which of the following is not part of the relational model?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "Each row or record in a table represents an instance of an entity type.\n",
    "\n",
    "Each column in a table represents an attribute or feature of an instance.\n",
    "Press\n",
    "\n",
    "\n",
    "Every table contains a primary key column, which has a unique entry for each row.\n",
    "Press\n",
    "\n",
    "\n",
    "ans: A database consists of at least 3 tables.\n",
    "\n",
    "There are relations between tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 0, 0, 3, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "x = [8, 'M', 'N', 3, 1, 'A']\n",
    "integers = [j if type(j) == int else 0 for j in x]\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1935287518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Creating a database engine\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Creating a database engine\n",
    "Here, you're going to fire up your very first SQL engine. You'll create an engine to connect to the SQLite database 'Chinook.sqlite', which is in your working directory. Remember that to create an engine to connect to 'Northwind.sqlite', Hugo executed the command\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "Here, 'sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite. A little bit of background on the Chinook database: the Chinook database contains information about a semi-fictional digital media store in which media data is real and customer, employee and sales data has been manually created.\n",
    "\n",
    "Why the name Chinook, you ask? According to their website,\n",
    "\n",
    "The name of this sample database was based on the Northwind database. Chinooks are winds in the interior West of North America, where the Canadian Prairies and Great Plains meet various mountain ranges. Chinooks are most prevalent over southern Alberta in Canada. Chinook is a good name choice for a database that intends to be an alternative to Northwind.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function create_engine from the module sqlalchemy.\n",
    "Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "\n",
    "# Import necessary module\n",
    "from ____ import ____\n",
    "\n",
    "# Create engine: engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite engine created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "print(\"SQLite engine created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the tables in the database?\n",
    "In this exercise, you'll once again create an engine to connect to 'Chinook.sqlite'. Before you can get any data out of the database, however, you'll need to know what tables it contains!\n",
    "\n",
    "To this end, you'll save the table names to a list using the method .table_names() on the engine and then you will print the list.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function create_engine from the module sqlalchemy.\n",
    "Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "Using the method .table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.\n",
    "Print the object table_names to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary module\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the table names to the shell\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m____\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '____' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(____)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'table_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///Chinook.sqlite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Save the table names to a list\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m table_names \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_names\u001b[49m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the table names to the shell\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(table_names)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Engine' object has no attribute 'table_names'"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary module\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get table names\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World of SQL Queries!\n",
    "Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, SELECT, in order to retrieve all columns of the table Album in the Chinook database. Recall that the query SELECT * selects all columns.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Open the engine connection as con using the method .connect() on the engine.\n",
    "Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "Store all of your query results in the DataFrame df by applying the .fetchall() method to the results rs.\n",
    "Close the connection!\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(____)\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(____)\n",
    "\n",
    "# Close connection\n",
    "\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'SELECT * FROM Album'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Open engine connection: con\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Perform query: rs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM Album\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Save results of the query to DataFrame: df\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchall(), columns\u001b[38;5;241m=\u001b[39mrs\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'SELECT * FROM Album'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "with engine.connect() as con:\n",
    "    # Perform query: rs\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "    # Save results of the query to DataFrame: df\n",
    "    df = pd.DataFrame(rs.fetchall(), columns=rs.keys())\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'SELECT * FROM Album'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m con \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Perform query: rs\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM Album\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Save results of the query to DataFrame: df\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchall())\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'SELECT * FROM Album'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "# data camp approach\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing the Hello World of SQL Queries\n",
    "Congratulations on executing your first SQL query! Now you're going to figure out how to customize your query in order to:\n",
    "\n",
    "Select specified columns from a table;\n",
    "Select a specified number of rows;\n",
    "Import column names from the database table.\n",
    "Recall that Hugo performed a very similar query customization in the video:\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "    df.columns = rs.keys()\n",
    "Packages have already been imported as follows:\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "The engine has also already been created:\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "The engine connection is already open with the statement\n",
    "\n",
    "with engine.connect() as con:\n",
    "All the code you need to complete is within this context.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.\n",
    "Apply the method fetchmany() to rs in order to retrieve 3 of the records. Store them in the DataFrame df.\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = ____\n",
    "    df = pd.DataFrame(____)\n",
    "    df.columns = ____\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'SELECT LastName, Title FROM Employee'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Open engine in context manager\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Perform query and save results to DataFrame: df\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Execute query selecting LastName and Title from Employee table\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT LastName, Title FROM Employee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ✅ Correct SQL query\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Fetch only 3 records\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchmany(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# ✅ Fetching only 3 rows\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'SELECT LastName, Title FROM Employee'"
     ]
    }
   ],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    # Execute query selecting LastName and Title from Employee table\n",
    "    rs = con.execute(\"SELECT LastName, Title FROM Employee\")  # ✅ Correct SQL query\n",
    "\n",
    "    # Fetch only 3 records\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))  # ✅ Fetching only 3 rows\n",
    "\n",
    "    # Set DataFrame column names using rs.keys()\n",
    "    df.columns = rs.keys()  # ✅ Setting correct column names\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (2989429154.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Filtering your database records using SQL's WHERE\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "Filtering your database records using SQL's WHERE\n",
    "You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query customizations to select particular columns and numbers of rows.\n",
    "\n",
    "There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.\n",
    "\n",
    "Let's say, for example that you wanted to get all records from the Customer table of the Chinook database for which the Country is 'Canada'. You can do this very easily in SQL using a SELECT statement followed by a WHERE clause as follows:\n",
    "\n",
    "SELECT * FROM Customer WHERE Country = 'Canada'\n",
    "In fact, you can filter any SELECT statement by any condition using a WHERE clause. This is called filtering your records.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table for which 'EmployeeId' is greater than or equal to 6.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Query away!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.\n",
    "Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= operator and assign the results to rs.\n",
    "Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(____)\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(____)\n",
    "    df = pd.DataFrame(____)\n",
    "    df.columns = ____\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'SELECT * FROM Employee WHERE EmployeeId >= 6'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Open engine in context manager\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform query and save results to DataFrame: df\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Execute query to select Employee records where EmployeeId >= 6\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM Employee WHERE EmployeeId >= 6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ✅ Correct SQL query\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Fetch all records and store them in a DataFrame\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchall())  \u001b[38;5;66;03m# ✅ Using fetchall()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'SELECT * FROM Employee WHERE EmployeeId >= 6'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')  # ✅ Correctly creating engine\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    # Execute query to select Employee records where EmployeeId >= 6\n",
    "    rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeId >= 6\")  # ✅ Correct SQL query\n",
    "\n",
    "    # Fetch all records and store them in a DataFrame\n",
    "    df = pd.DataFrame(rs.fetchall())  # ✅ Using fetchall()\n",
    "\n",
    "    # Set column names using rs.keys()\n",
    "    df.columns = rs.keys()  # ✅ Assigning correct column names\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Load the Employee data into a DataFrame (replace this with your actual DataFrame)\n",
    "data = {\n",
    "    \"EmployeeId\": [1, 2, 3, 4, 5],\n",
    "    \"LastName\": [\"Adams\", \"Edwards\", \"Peacock\", \"Park\", \"Johnson\"],\n",
    "    \"FirstName\": [\"Andrew\", \"Nancy\", \"Jane\", \"Margaret\", \"Steve\"],\n",
    "    \"Title\": [\"General Manager\", \"Sales Manager\", \"Sales Support Agent\", \"Sales Support Agent\", \"Sales Support Agent\"],\n",
    "    \"ReportsTo\": [None, 1, 2, 2, 2],\n",
    "    \"Country\": [\"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\"],\n",
    "    \"PostalCode\": [\"T5K 2N1\", \"T2P 2T3\", \"T2P 5M5\", \"T2P 5G3\", \"T3B 1Y7\"],\n",
    "    \"Phone\": [\"+1 (780) 428-9482\", \"+1 (403) 262-3443\", \"+1 (403) 262-3443\", \"+1 (403) 263-4423\", \"1 (780) 836-9987\"],\n",
    "    \"Fax\": [\"+1 (780) 428-3457\", \"+1 (403) 262-3322\", \"+1 (403) 262-6712\", \"+1 (403) 263-4289\", \"1 (780) 836-9543\"],\n",
    "    \"Email\": [\"andrew@chinookcorp.com\", \"nancy@chinookcorp.com\", \"jane@chinookcorp.com\", \"margaret@chinookcorp.com\", \"steve@chinookcorp.com\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Insert data into the Employee table\n",
    "df.to_sql(\"Employee\", con=engine, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"✅ Data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (1607459070.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    In this interactive exercise, you'll select all records of the Employee table and order them in increasing order by the column BirthDate.\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "Ordering your SQL records with ORDER BY\n",
    "You can also order your SQL query results. For example, if you wanted to get all records from the Customer table of the Chinook database and order them in increasing order by the column SupportRepId, you could do so with the following query:\n",
    "\n",
    "\"SELECT * FROM Customer ORDER BY SupportRepId\"\n",
    "In fact, you can order any SELECT statement by any column.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table and order them in increasing order by the column BirthDate.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Get querying!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "In the context manager, execute the query that selects all records from the Employee table and orders them in increasing order by the column BirthDate. Assign the result to rs.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = ____\n",
    "    df = ____\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: 'SELECT * FROM Employee ORDER BY BirthDate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Open engine in context manager\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Execute query: Select all employees ordered by BirthDate\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM Employee ORDER BY BirthDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Fetch all results and store in a DataFrame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchall())\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: 'SELECT * FROM Employee ORDER BY BirthDate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    # Execute query: Select all employees ordered by BirthDate\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
    "\n",
    "    # Fetch all results and store in a DataFrame\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and The Hello World of SQL Queries!\n",
    "Here, you'll take advantage of the power of pandas to write the results of your SQL query to a DataFrame in one swift line of Python code!\n",
    "\n",
    "You'll first import pandas and create the SQLite 'Chinook.sqlite' engine. Then you'll query the database to select all records from the Album table.\n",
    "\n",
    "Recall that to select all records from the Orders table in the Northwind database, Hugo executed the following command:\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pandas package using the alias pd.\n",
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the table Album.\n",
    "The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous method that you learned.\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import ____ as ____\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(____, ____)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such column: BirthDate\n[SQL: \nSELECT * FROM Employee\nWHERE EmployeeId >= 6\nORDER BY BirthDate\n]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such column: BirthDate",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 14\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Execute query and store records in DataFrame: df\u001b[39;00m\n\u001b[0;32m      9\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124mSELECT * FROM Employee\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124mWHERE EmployeeId >= 6\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mORDER BY BirthDate\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Print head of DataFrame\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:1836\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1781\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1788\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1834\u001b[0m \n\u001b[0;32m   1835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1836\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1837\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:1659\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   1657\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1779\u001b[0m, in \u001b[0;36mConnection.exec_driver_sql\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1774\u001b[0m execution_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_options\u001b[38;5;241m.\u001b[39mmerge_with(\n\u001b[0;32m   1775\u001b[0m     execution_options\n\u001b[0;32m   1776\u001b[0m )\n\u001b[0;32m   1778\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[1;32m-> 1779\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2355\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2354\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2357\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1974\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1979\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such column: BirthDate\n[SQL: \nSELECT * FROM Employee\nWHERE EmployeeId >= 6\nORDER BY BirthDate\n]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "query = \"\"\"\n",
    "SELECT * FROM Employee\n",
    "WHERE EmployeeId >= 6\n",
    "ORDER BY BirthDate\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "query = \"\"\"\n",
    "SELECT * FROM Employee\n",
    "WHERE EmployeeId >= 6\n",
    "ORDER BY BirthDate\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of SQL lies in relationships between tables: INNER JOIN\n",
    "Here, you'll perform your first INNER JOIN! You'll be working with your favourite SQLite database, Chinook.sqlite. For each record in the Album table, you'll extract the Title along with the Name of the Artist. The latter will come from the Artist table and so you will need to INNER JOIN these two tables on the ArtistID column of both.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "Instructions\n",
    "100 XP\n",
    "Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    ____\n",
    "    ____\n",
    "    ____\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ObjectNotExecutableError",
     "evalue": "Not an executable object: '\\n        SELECT Album.Title, Artist.Name \\n        FROM Album \\n        INNER JOIN Artist \\n        ON Album.ArtistId = Artist.ArtistId\\n    '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1414\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[43mstatement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_on_connection\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute '_execute_on_connection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Open engine in context manager\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Perform query and save results to DataFrame: df\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[1;32m----> 4\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43m        SELECT Album.Title, Artist.Name \u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43m        FROM Album \u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43m        INNER JOIN Artist \u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43m        ON Album.ArtistId = Artist.ArtistId\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rs\u001b[38;5;241m.\u001b[39mfetchall())  \u001b[38;5;66;03m# Fetch all records into a DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mkeys()  \u001b[38;5;66;03m# Set column names\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adams\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1416\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     meth \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39m_execute_on_connection\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[0;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1420\u001b[0m         distilled_parameters,\n\u001b[0;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[0;32m   1422\u001b[0m     )\n",
      "\u001b[1;31mObjectNotExecutableError\u001b[0m: Not an executable object: '\\n        SELECT Album.Title, Artist.Name \\n        FROM Album \\n        INNER JOIN Artist \\n        ON Album.ArtistId = Artist.ArtistId\\n    '"
     ]
    }
   ],
   "source": [
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"\"\"\n",
    "        SELECT Album.Title, Artist.Name \n",
    "        FROM Album \n",
    "        INNER JOIN Artist \n",
    "        ON Album.ArtistId = Artist.ArtistId\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.DataFrame(rs.fetchall())  # Fetch all records into a DataFrame\n",
    "    df.columns = rs.keys()  # Set column names\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * \n",
    "    FROM PlaylistTrack \n",
    "    INNER JOIN Track \n",
    "    ON PlaylistTrack.TrackId = Track.TrackId\n",
    "    WHERE Milliseconds < 250000\n",
    "\"\"\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the correct Python code to generate the required output:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "matrix = [[i + 1 for i in range(3)] for _ in range(3)]\n",
    "print(matrix)\n",
    "Explanation:\n",
    "The inner list comprehension [i + 1 for i in range(3)] generates [1, 2, 3].\n",
    "The outer list comprehension repeats this list three times to form the final matrix.\n",
    "Output:\n",
    "plaintext\n",
    "Copy\n",
    "Edit\n",
    "[[1, 2, 3], [1, 2, 3], [1, 2, 3]]\n",
    "Let me know if you need further clarification! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how can i create sqlite engine using python in my PC, and I dont have SQLlite installed\n",
    "ChatGPT said:\n",
    "You don’t need to install SQLite separately because Python comes with an in-built SQLite library. You can create an SQLite engine using SQLAlchemy or the built-in sqlite3 module.\n",
    "\n",
    "Method 1: Using sqlite3 (Built-in)\n",
    "This is the simplest way to create an SQLite database.\n",
    "\n",
    "Steps to Create SQLite Engine Using sqlite3:\n",
    "Open a terminal or command prompt.\n",
    "Run the following Python script:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import sqlite3\n",
    "\n",
    "# Create a database file (or connect if it exists)\n",
    "conn = sqlite3.connect(\"my_database.db\")\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a sample table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        age INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Commit and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"SQLite database and table created successfully!\")\n",
    "This will create a file named my_database.db in the current directory.\n",
    "If the file already exists, it will connect to it.\n",
    "Method 2: Using SQLAlchemy\n",
    "If you want to use an SQLAlchemy engine (which is useful for larger projects), install SQLAlchemy first:\n",
    "\n",
    "sh\n",
    "Copy\n",
    "Edit\n",
    "pip install sqlalchemy\n",
    "Steps to Create SQLite Engine Using SQLAlchemy:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an SQLite engine (this will create 'my_database.db' file)\n",
    "engine = create_engine(\"sqlite:///my_database.db\")\n",
    "\n",
    "print(\"SQLite engine created successfully!\")\n",
    "sqlite:///my_database.db tells SQLAlchemy to use SQLite and create my_database.db in the current directory.\n",
    "Checking if SQLite Works\n",
    "To check if SQLite is working, run:\n",
    "\n",
    "sh\n",
    "Copy\n",
    "Edit\n",
    "python -c \"import sqlite3; print(sqlite3.sqlite_version)\"\n",
    "If SQLite is working, it will return the SQLite version installed with Python.\n",
    "\n",
    "Let me know if you have any issues! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Creating a database engine\n",
    "Here, you're going to fire up your very first SQL engine. You'll create an engine to connect to the SQLite database 'Chinook.sqlite', which is in your working directory. Remember that to create an engine to connect to 'Northwind.sqlite', Hugo executed the command\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "Here, 'sqlite:///Northwind.sqlite' is called the connection string to the SQLite database Northwind.sqlite. A little bit of background on the Chinook database: the Chinook database contains information about a semi-fictional digital media store in which media data is real and customer, employee and sales data has been manually created.\n",
    "\n",
    "Why the name Chinook, you ask? According to their website,\n",
    "\n",
    "The name of this sample database was based on the Northwind database. Chinooks are winds in the interior West of North America, where the Canadian Prairies and Great Plains meet various mountain ranges. Chinooks are most prevalent over southern Alberta in Canada. Chinook is a good name choice for a database that intends to be an alternative to Northwind.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function create_engine from the module sqlalchemy.\n",
    "Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "\n",
    "# Import necessary module\n",
    "from ____ import ____\n",
    "\n",
    "# Create engine: engine\n",
    "ChatGPT said:\n",
    "Here’s the correct code to create the SQLite engine for the Chinook database using SQLAlchemy:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "print(\"SQLite engine created successfully!\")\n",
    "Explanation:\n",
    "Import create_engine: This function is used to create a database connection.\n",
    "Create an engine: 'sqlite:///Chinook.sqlite' is the connection string for the SQLite database named Chinook.sqlite.\n",
    "Assign to engine: The engine acts as a bridge between Python and the SQLite database.\n",
    "After running this, an SQLite engine will be ready to execute queries.\n",
    "\n",
    "Would you like to run some queries on this database? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "What are the tables in the database?\n",
    "In this exercise, you'll once again create an engine to connect to 'Chinook.sqlite'. Before you can get any data out of the database, however, you'll need to know what tables it contains!\n",
    "\n",
    "To this end, you'll save the table names to a list using the method .table_names() on the engine and then you will print the list.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the function create_engine from the module sqlalchemy.\n",
    "Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "Using the method .table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.\n",
    "Print the object table_names to the shell.\n",
    "\n",
    "# Import necessary module\n",
    "\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here’s the correct code to get the table names from the Chinook database using SQLAlchemy:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list\n",
    "table_names = engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n",
    "Explanation:\n",
    "Import create_engine: Required for connecting to the SQLite database.\n",
    "Create the engine: 'sqlite:///Chinook.sqlite' establishes a connection to the database.\n",
    "Retrieve table names: .table_names() fetches all table names from the database.\n",
    "Print the tables: Displays the list of tables in the Chinook database.\n",
    "Note:\n",
    "If you're using SQLAlchemy version 1.4+, .table_names() is deprecated. Use this instead:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "# Get table names using the Inspector\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "print(table_names)\n",
    "Let me know if you need further help! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "'Engine' object has no attribute 'table_names'\n",
    "ChatGPT said:\n",
    "You're right! In SQLAlchemy version 1.4+, the .table_names() method has been deprecated. Instead, we use the Inspector to get the table names.\n",
    "\n",
    "Correct Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary module\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Create an inspector object\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get table names\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n",
    "Explanation:\n",
    "Import inspect: Used to inspect database structures.\n",
    "Create the engine: Connects to the SQLite database.\n",
    "Use inspect(engine): Gets database metadata.\n",
    "Retrieve table names: .get_table_names() returns a list of table names.\n",
    "Now, when you run the script, it should correctly print the tables in Chinook.sqlite. Let me know if you have any issues! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "The Hello World of SQL Queries!\n",
    "Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, SELECT, in order to retrieve all columns of the table Album in the Chinook database. Recall that the query SELECT * selects all columns.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Open the engine connection as con using the method .connect() on the engine.\n",
    "Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "Store all of your query results in the DataFrame df by applying the .fetchall() method to the results rs.\n",
    "Close the connection!\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(____)\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(____)\n",
    "\n",
    "# Close connection\n",
    "\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's the correct code to perform a SELECT query on the Album table in the Chinook database:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "with engine.connect() as con:\n",
    "    # Perform query: rs\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "    # Save results of the query to DataFrame: df\n",
    "    df = pd.DataFrame(rs.fetchall(), columns=rs.keys())\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "Explanation:\n",
    "Import necessary packages:\n",
    "\n",
    "create_engine from sqlalchemy to connect to the SQLite database.\n",
    "pandas for handling data in a DataFrame.\n",
    "Create a connection engine:\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite') creates a connection to the database.\n",
    "Open connection using with statement:\n",
    "\n",
    "with engine.connect() as con: ensures the connection closes automatically after execution.\n",
    "Execute SQL query:\n",
    "\n",
    "rs = con.execute(\"SELECT * FROM Album\") retrieves all rows and columns from the Album table.\n",
    "Convert to Pandas DataFrame:\n",
    "\n",
    "rs.fetchall() fetches all results.\n",
    "columns=rs.keys() ensures column names are preserved.\n",
    "Print the first few rows:\n",
    "\n",
    "df.head() displays the first few records.\n",
    "Output (Example)\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "   AlbumId                      Title  ArtistId\n",
    "0        1  For Those About To Rock          1\n",
    "1        2              Balls to the Wall      2\n",
    "2        3               Restless and Wild    2\n",
    "3        4                 Let There Be Rock  1\n",
    "4        5                       Big Ones     3\n",
    "This correctly retrieves data from the Album table. Let me know if you have any questions! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Incorrect submission\n",
    "Did you call pd.DataFrame()?\n",
    "ChatGPT said:\n",
    "Your code is mostly correct, but let's ensure that pd.DataFrame() is called properly. Here's the fully correct version:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "# Fetch all rows\n",
    "data = rs.fetchall()\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "df = pd.DataFrame(data, columns=rs.keys())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "Fixes & Explanation:\n",
    "Did you call pd.DataFrame()? ✅\n",
    "\n",
    "Yes! df = pd.DataFrame(data, columns=rs.keys()) ensures the results are converted into a DataFrame with correct column names.\n",
    "Ensure .fetchall() is assigned to a variable ✅\n",
    "\n",
    "data = rs.fetchall() correctly extracts the rows before using pd.DataFrame().\n",
    "Make sure the connection is closed properly ✅\n",
    "\n",
    "con.close() ensures that the database connection is closed after execution.\n",
    "Expected Output (Example):\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "   AlbumId                      Title  ArtistId\n",
    "0        1  For Those About To Rock          1\n",
    "1        2              Balls to the Wall      2\n",
    "2        3               Restless and Wild    2\n",
    "3        4                 Let There Be Rock  1\n",
    "4        5                       Big Ones     3\n",
    "Now, this should pass your submission! 🚀 Let me know if you need more help.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Incorrect submission\n",
    "Check your call of pd.DataFrame(). Did you correctly specify the first argument? Did you call rs.fetchall()?\n",
    "ChatGPT said:\n",
    "It looks like the platform wants you to explicitly call rs.fetchall() inside pd.DataFrame() and ensure that the first argument is the query result. Let's correct this:\n",
    "\n",
    "✅ Corrected Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())  # ✅ Correct first argument\n",
    "\n",
    "# Set column names using rs.keys()\n",
    "df.columns = rs.keys()\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "🔹 Key Fixes:\n",
    "pd.DataFrame(rs.fetchall()) is called directly ✅\n",
    "\n",
    "Ensures that rs.fetchall() is the first argument to pd.DataFrame().\n",
    "Set column names separately using df.columns = rs.keys() ✅\n",
    "\n",
    "This ensures that column names are correctly assigned after creating the DataFrame.\n",
    "Closes the connection properly ✅\n",
    "\n",
    "con.close() ensures database resources are released.\n",
    "🎯 Expected Output (Example)\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "   AlbumId                      Title  ArtistId\n",
    "0        1  For Those About To Rock          1\n",
    "1        2              Balls to the Wall      2\n",
    "2        3               Restless and Wild    2\n",
    "3        4                 Let There Be Rock  1\n",
    "4        5                       Big Ones     3\n",
    "Now this should work perfectly for your submission. Let me know if you run into any issues! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "ncorrect submission\n",
    "Did you correctly define the pandas DataFrame df? Expected something different.\n",
    "ChatGPT said:\n",
    "It looks like the platform is expecting an explicit definition of the DataFrame using pd.DataFrame(), ensuring that the correct arguments are passed in the right order. Let's make sure everything is exactly as expected:\n",
    "\n",
    "✅ Final Correct Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall(), columns=rs.keys())  # ✅ Correctly defining the DataFrame\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "🔹 Key Fixes:\n",
    "pd.DataFrame(rs.fetchall(), columns=rs.keys()) ✅\n",
    "\n",
    "Ensures fetchall() is the first argument (data).\n",
    "Ensures columns=rs.keys() is set correctly.\n",
    "Closes the connection properly (con.close()) ✅\n",
    "\n",
    "This is necessary for database resource management.\n",
    "🎯 Expected Output (Example)\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "   AlbumId                      Title  ArtistId\n",
    "0        1  For Those About To Rock          1\n",
    "1        2              Balls to the Wall      2\n",
    "2        3               Restless and Wild    2\n",
    "3        4                 Let There Be Rock  1\n",
    "4        5                       Big Ones     3\n",
    "This exact structure should now pass your submission. 🚀\n",
    "Let me know if you still face any issues! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Hint\n",
    "Did you use the method .connect() on engine?\n",
    "Pass the query as an argument to con.execute().\n",
    "Use rs.fetchall() as an argument to pd.DataFrame().\n",
    "Recall that you can use the .close() method on the engine connection to close the connection.\n",
    "ChatGPT said:\n",
    "Thanks for the hint! Let's ensure we exactly follow the expected structure.\n",
    "\n",
    "✅ Final Correct Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()  # ✅ Using .connect() on engine\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute(\"SELECT * FROM Album\")  # ✅ Passing query to con.execute()\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())  # ✅ Using rs.fetchall() as argument to pd.DataFrame()\n",
    "\n",
    "# Set column names\n",
    "df.columns = rs.keys()\n",
    "\n",
    "# Close connection\n",
    "con.close()  # ✅ Using .close() on connection\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "🔹 Key Fixes Based on the Hint:\n",
    "Did you use .connect() on engine? ✅\n",
    "\n",
    "con = engine.connect() ensures a valid connection.\n",
    "Pass the query to con.execute() ✅\n",
    "\n",
    "rs = con.execute(\"SELECT * FROM Album\") correctly executes the query.\n",
    "Use rs.fetchall() as an argument to pd.DataFrame() ✅\n",
    "\n",
    "df = pd.DataFrame(rs.fetchall()) ensures results are stored properly.\n",
    "Use .close() on the connection ✅\n",
    "\n",
    "con.close() releases the database connection.\n",
    "🎯 Expected Output (Example)\n",
    "vbnet\n",
    "Copy\n",
    "Edit\n",
    "   AlbumId                      Title  ArtistId\n",
    "0        1  For Those About To Rock          1\n",
    "1        2              Balls to the Wall      2\n",
    "2        3               Restless and Wild    2\n",
    "3        4                 Let There Be Rock  1\n",
    "4        5                       Big Ones     3\n",
    "This exactly matches the required format and should now pass your submission! 🚀\n",
    "Let me know if you run into any issues! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Customizing the Hello World of SQL Queries\n",
    "Congratulations on executing your first SQL query! Now you're going to figure out how to customize your query in order to:\n",
    "\n",
    "Select specified columns from a table;\n",
    "Select a specified number of rows;\n",
    "Import column names from the database table.\n",
    "Recall that Hugo performed a very similar query customization in the video:\n",
    "\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "    df.columns = rs.keys()\n",
    "Packages have already been imported as follows:\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "The engine has also already been created:\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "The engine connection is already open with the statement\n",
    "\n",
    "with engine.connect() as con:\n",
    "All the code you need to complete is within this context.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Execute the SQL query that selects the columns LastName and Title from the Employee table. Store the results in the variable rs.\n",
    "Apply the method fetchmany() to rs in order to retrieve 3 of the records. Store them in the DataFrame df.\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = ____\n",
    "    df = pd.DataFrame(____)\n",
    "    df.columns = ____\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's the correct code to customize the SQL query and retrieve specific columns from the Employee table:\n",
    "\n",
    "✅ Final Code\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    # Execute query selecting LastName and Title from Employee table\n",
    "    rs = con.execute(\"SELECT LastName, Title FROM Employee\")  # ✅ Correct SQL query\n",
    "\n",
    "    # Fetch only 3 records\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))  # ✅ Fetching only 3 rows\n",
    "\n",
    "    # Set DataFrame column names using rs.keys()\n",
    "    df.columns = rs.keys()  # ✅ Setting correct column names\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "🔹 Key Fixes Based on the Instructions:\n",
    "Did you select only the required columns? ✅\n",
    "\n",
    "\"SELECT LastName, Title FROM Employee\" ensures only LastName and Title are retrieved.\n",
    "Did you fetch only 3 records? ✅\n",
    "\n",
    "rs.fetchmany(size=3) ensures only 3 rows are returned.\n",
    "Did you set the column names correctly? ✅\n",
    "\n",
    "df.columns = rs.keys() assigns the correct column names.\n",
    "🎯 Expected Output (Example)\n",
    "mathematica\n",
    "Copy\n",
    "Edit\n",
    "3\n",
    "  LastName            Title\n",
    "0  Adams     General Manager\n",
    "1  Edwards   Sales Manager\n",
    "2  Park      IT Manager\n",
    "🚀 This should now pass your submission! 🚀\n",
    "Let me know if you need any modifications. 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Filtering your database records using SQL's WHERE\n",
    "You can now execute a basic SQL query to select records from any table in your database and you can also perform simple query customizations to select particular columns and numbers of rows.\n",
    "\n",
    "There are a couple more standard SQL query chops that will aid you in your journey to becoming an SQL ninja.\n",
    "\n",
    "Let's say, for example that you wanted to get all records from the Customer table of the Chinook database for which the Country is 'Canada'. You can do this very easily in SQL using a SELECT statement followed by a WHERE clause as follows:\n",
    "\n",
    "SELECT * FROM Customer WHERE Country = 'Canada'\n",
    "In fact, you can filter any SELECT statement by any condition using a WHERE clause. This is called filtering your records.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table for which 'EmployeeId' is greater than or equal to 6.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Query away!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the argument of create_engine() so that the engine for the SQLite database 'Chinook.sqlite' is created.\n",
    "Execute the query that selects all records from the Employee table where 'EmployeeId' is greater than or equal to 6. Use the >= operator and assign the results to rs.\n",
    "Apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Using the rs object, set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine(____)\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(____)\n",
    "    df = pd.DataFrame(____)\n",
    "    df.columns = ____\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's the correct code to filter records from the Employee table where EmployeeId >= 6:\n",
    "\n",
    "✅ Final Code\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')  # ✅ Correctly creating engine\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    # Execute query to select Employee records where EmployeeId >= 6\n",
    "    rs = con.execute(\"SELECT * FROM Employee WHERE EmployeeId >= 6\")  # ✅ Correct SQL query\n",
    "\n",
    "    # Fetch all records and store them in a DataFrame\n",
    "    df = pd.DataFrame(rs.fetchall())  # ✅ Using fetchall()\n",
    "\n",
    "    # Set column names using rs.keys()\n",
    "    df.columns = rs.keys()  # ✅ Assigning correct column names\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "🔹 Key Fixes Based on Instructions:\n",
    "Did you correctly define create_engine()? ✅\n",
    "\n",
    "engine = create_engine('sqlite:///Chinook.sqlite') correctly initializes the database connection.\n",
    "Did you filter using WHERE EmployeeId >= 6? ✅\n",
    "\n",
    "\"SELECT * FROM Employee WHERE EmployeeId >= 6\" ensures only the required rows are selected.\n",
    "Did you fetch all records with fetchall()? ✅\n",
    "\n",
    "df = pd.DataFrame(rs.fetchall()) retrieves all matching rows.\n",
    "Did you set the column names correctly? ✅\n",
    "\n",
    "df.columns = rs.keys() ensures the DataFrame has the correct column headers.\n",
    "🎯 Expected Output (Example)\n",
    "yaml\n",
    "Copy\n",
    "Edit\n",
    "   EmployeeId LastName FirstName  Title              BirthDate         HireDate\n",
    "0          6   Roberts   Tom     IT Manager        1977-06-03     2005-01-02\n",
    "1          7   King      Samantha Sales Support    1985-09-12     2010-03-15\n",
    "2          8   Adams     Robert  HR Manager        1980-07-20     2008-06-30\n",
    "🚀 This should now pass your submission! 🚀\n",
    "Let me know if you need any help! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how would I add this in my database engine:\n",
    "EmployeeId LastName FirstName                Title  ReportsTo  ... Country PostalCode              Phone                Fax                     Email\n",
    "0           1    Adams    Andrew      General Manager        NaN  ...  Canada    T5K 2N1  +1 (780) 428-9482  +1 (780) 428-3457    andrew@chinookcorp.com\n",
    "1           2  Edwards     Nancy        Sales Manager        1.0  ...  Canada    T2P 2T3  +1 (403) 262-3443  +1 (403) 262-3322     nancy@chinookcorp.com\n",
    "2           3  Peacock      Jane  Sales Support Agent        2.0  ...  Canada    T2P 5M5  +1 (403) 262-3443  +1 (403) 262-6712      jane@chinookcorp.com\n",
    "3           4     Park  Margaret  Sales Support Agent        2.0  ...  Canada    T2P 5G3  +1 (403) 263-4423  +1 (403) 263-4289  margaret@chinookcorp.com\n",
    "4           5  Johnson     Steve  Sales Support Agent        2.0  ...  Canada    T3B 1Y7   1 (780) 836-9987   1 (780) 836-9543     steve@chinookcorp.com\n",
    "ChatGPT said:\n",
    "To insert this data into your SQLite database (Chinook.sqlite), follow these steps:\n",
    "\n",
    "✅ 1. Convert the DataFrame into SQL Table\n",
    "If you have the data in a Pandas DataFrame (df), you can use the .to_sql() method to insert it into the Employee table.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Load the Employee data into a DataFrame (replace this with your actual DataFrame)\n",
    "data = {\n",
    "    \"EmployeeId\": [1, 2, 3, 4, 5],\n",
    "    \"LastName\": [\"Adams\", \"Edwards\", \"Peacock\", \"Park\", \"Johnson\"],\n",
    "    \"FirstName\": [\"Andrew\", \"Nancy\", \"Jane\", \"Margaret\", \"Steve\"],\n",
    "    \"Title\": [\"General Manager\", \"Sales Manager\", \"Sales Support Agent\", \"Sales Support Agent\", \"Sales Support Agent\"],\n",
    "    \"ReportsTo\": [None, 1, 2, 2, 2],\n",
    "    \"Country\": [\"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\"],\n",
    "    \"PostalCode\": [\"T5K 2N1\", \"T2P 2T3\", \"T2P 5M5\", \"T2P 5G3\", \"T3B 1Y7\"],\n",
    "    \"Phone\": [\"+1 (780) 428-9482\", \"+1 (403) 262-3443\", \"+1 (403) 262-3443\", \"+1 (403) 263-4423\", \"1 (780) 836-9987\"],\n",
    "    \"Fax\": [\"+1 (780) 428-3457\", \"+1 (403) 262-3322\", \"+1 (403) 262-6712\", \"+1 (403) 263-4289\", \"1 (780) 836-9543\"],\n",
    "    \"Email\": [\"andrew@chinookcorp.com\", \"nancy@chinookcorp.com\", \"jane@chinookcorp.com\", \"margaret@chinookcorp.com\", \"steve@chinookcorp.com\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Insert data into the Employee table\n",
    "df.to_sql(\"Employee\", con=engine, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"✅ Data inserted successfully!\")\n",
    "✅ 2. If Table Does Not Exist, Create It First\n",
    "If you haven't created the Employee table in the Chinook database yet, you need to do it manually before inserting the data.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Create database engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# SQL query to create Employee table if not exists\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Employee (\n",
    "    EmployeeId INTEGER PRIMARY KEY,\n",
    "    LastName TEXT,\n",
    "    FirstName TEXT,\n",
    "    Title TEXT,\n",
    "    ReportsTo INTEGER,\n",
    "    Country TEXT,\n",
    "    PostalCode TEXT,\n",
    "    Phone TEXT,\n",
    "    Fax TEXT,\n",
    "    Email TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_table_query))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"✅ Employee table created (if not exists).\")\n",
    "✅ 3. Manually Insert Data Using SQL Queries\n",
    "If you want to insert data manually row by row instead of using Pandas, use SQL INSERT statements:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Insert data manually into Employee table\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO Employee (EmployeeId, LastName, FirstName, Title, ReportsTo, Country, PostalCode, Phone, Fax, Email)\n",
    "VALUES\n",
    "(1, 'Adams', 'Andrew', 'General Manager', NULL, 'Canada', 'T5K 2N1', '+1 (780) 428-9482', '+1 (780) 428-3457', 'andrew@chinookcorp.com'),\n",
    "(2, 'Edwards', 'Nancy', 'Sales Manager', 1, 'Canada', 'T2P 2T3', '+1 (403) 262-3443', '+1 (403) 262-3322', 'nancy@chinookcorp.com'),\n",
    "(3, 'Peacock', 'Jane', 'Sales Support Agent', 2, 'Canada', 'T2P 5M5', '+1 (403) 262-3443', '+1 (403) 262-6712', 'jane@chinookcorp.com'),\n",
    "(4, 'Park', 'Margaret', 'Sales Support Agent', 2, 'Canada', 'T2P 5G3', '+1 (403) 263-4423', '+1 (403) 263-4289', 'margaret@chinookcorp.com'),\n",
    "(5, 'Johnson', 'Steve', 'Sales Support Agent', 2, 'Canada', 'T3B 1Y7', '1 (780) 836-9987', '1 (780) 836-9543', 'steve@chinookcorp.com');\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(insert_query))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"✅ Employee data inserted successfully!\")\n",
    "🎯 Which Method Should You Use?\n",
    "Method\tUse Case\n",
    "to_sql() (Pandas)\tIf your data is in a DataFrame and you want to insert bulk records easily.\n",
    "CREATE TABLE + INSERT (SQL Queries)\tIf the table does not exist or you want to insert data manually row by row.\n",
    "🚀 Now your database engine will contain the Employee records! Let me know if you need help. 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Ordering your SQL records with ORDER BY\n",
    "You can also order your SQL query results. For example, if you wanted to get all records from the Customer table of the Chinook database and order them in increasing order by the column SupportRepId, you could do so with the following query:\n",
    "\n",
    "\"SELECT * FROM Customer ORDER BY SupportRepId\"\n",
    "In fact, you can order any SELECT statement by any column.\n",
    "\n",
    "In this interactive exercise, you'll select all records of the Employee table and order them in increasing order by the column BirthDate.\n",
    "\n",
    "Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "Get querying!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "In the context manager, execute the query that selects all records from the Employee table and orders them in increasing order by the column BirthDate. Assign the result to rs.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = ____\n",
    "    df = ____\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here’s the correct way to write the code for ordering records in the Employee table by BirthDate:\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    # Execute query: Select all employees ordered by BirthDate\n",
    "    rs = con.execute(\"SELECT * FROM Employee ORDER BY BirthDate\")\n",
    "\n",
    "    # Fetch all results and store in a DataFrame\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "📝 Explanation\n",
    "Create the SQLite engine using create_engine('sqlite:///Chinook.sqlite').\n",
    "Execute SQL query: \"SELECT * FROM Employee ORDER BY BirthDate\".\n",
    "This selects all columns in Employee and sorts them by BirthDate (ascending by default).\n",
    "Fetch all results into a DataFrame using .fetchall().\n",
    "Assign column names to match the table structure using rs.keys().\n",
    "Print the DataFrame head to check the results.\n",
    "This will return employees ordered from the oldest to youngest by their birth date. 🎯🚀\n",
    "\n",
    "Let me know if you need any modifications! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Pandas and The Hello World of SQL Queries!\n",
    "Here, you'll take advantage of the power of pandas to write the results of your SQL query to a DataFrame in one swift line of Python code!\n",
    "\n",
    "You'll first import pandas and create the SQLite 'Chinook.sqlite' engine. Then you'll query the database to select all records from the Album table.\n",
    "\n",
    "Recall that to select all records from the Orders table in the Northwind database, Hugo executed the following command:\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pandas package using the alias pd.\n",
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the table Album.\n",
    "The remainder of the code is included to confirm that the DataFrame created by this method is equal to that created by the previous method that you learned.\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import ____ as ____\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(____, ____)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))\n",
    "ChatGPT said:\n",
    "Here’s the correct way to write the code using pandas' read_sql_query() function:\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd  # Import pandas with alias pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))\n",
    "📝 Explanation\n",
    "Import necessary libraries:\n",
    "\n",
    "sqlalchemy.create_engine to create a connection to the SQLite database.\n",
    "pandas as pd for working with data.\n",
    "Create the SQLite engine:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "This sets up the connection to the Chinook.sqlite database.\n",
    "\n",
    "Query the database using read_sql_query():\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_sql_query(\"SELECT * FROM Album\", engine)\n",
    "This runs the SQL query and directly returns a pandas DataFrame.\n",
    "Advantage: It simplifies the process and automatically assigns column names.\n",
    "Alternative method (manual fetch):\n",
    "\n",
    "We also fetch the data using con.execute(), convert it into a DataFrame, and assign column names manually.\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "Verify if both methods give the same output:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.equals(df1))\n",
    "If True, both methods return identical DataFrames.\n",
    "🎯 Key Takeaways\n",
    "✅ pd.read_sql_query() is more concise and efficient than manual fetching.\n",
    "✅ Using with engine.connect() ensures the connection is properly closed.\n",
    "✅ Both methods return the same result but read_sql_query() is recommended for ease of use.\n",
    "\n",
    "Let me know if you need further clarifications! 🚀😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Pandas for more complex querying\n",
    "Here, you'll become more familiar with the pandas function read_sql_query() by using it to execute a more complex query: a SELECT statement followed by both a WHERE clause AND an ORDER BY clause.\n",
    "\n",
    "You'll build a DataFrame that contains the rows of the Employee table for which the EmployeeId is greater than or equal to 6 and you'll order these entries by BirthDate.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using the function create_engine(), create an engine for the SQLite database Chinook.sqlite and assign it to the variable engine.\n",
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from the Employee table where the EmployeeId is greater than or equal to 6 and ordered by BirthDate (make sure to use WHERE and ORDER BY in this precise order).\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here’s how you can execute the complex SQL query using pandas.read_sql_query():\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "query = \"\"\"\n",
    "SELECT * FROM Employee \n",
    "WHERE EmployeeId >= 6 \n",
    "ORDER BY BirthDate\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "📝 Explanation\n",
    "Import necessary libraries:\n",
    "\n",
    "sqlalchemy.create_engine for database connection.\n",
    "pandas as pd for working with the DataFrame.\n",
    "Create the SQLite engine:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "This connects to the Chinook.sqlite database.\n",
    "Define the SQL query:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "query = \"\"\"\n",
    "SELECT * FROM Employee \n",
    "WHERE EmployeeId >= 6 \n",
    "ORDER BY BirthDate\n",
    "\"\"\"\n",
    "Selects all columns from the Employee table.\n",
    "Filters records where EmployeeId >= 6.\n",
    "Orders results by BirthDate in ascending order.\n",
    "Run the SQL query with pandas:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_sql_query(query, engine)\n",
    "This fetches the query result directly as a DataFrame.\n",
    "Print the first few rows:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.head())\n",
    "Displays the top records.\n",
    "🎯 Key Takeaways\n",
    "✅ pandas.read_sql_query() simplifies complex queries.\n",
    "✅ Using multi-line queries (\"\"\" \"\"\") improves readability.\n",
    "✅ SQL clauses should be ordered correctly: WHERE → ORDER BY.\n",
    "\n",
    "Let me know if you have any questions! 🚀😊\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "You said:\n",
    "The power of SQL lies in relationships between tables: INNER JOIN\n",
    "Here, you'll perform your first INNER JOIN! You'll be working with your favourite SQLite database, Chinook.sqlite. For each record in the Album table, you'll extract the Title along with the Name of the Artist. The latter will come from the Artist table and so you will need to INNER JOIN these two tables on the ArtistID column of both.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "Instructions\n",
    "100 XP\n",
    "Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    ____\n",
    "    ____\n",
    "    ____\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's how you can perform an INNER JOIN between the Album and Artist tables to retrieve the Title and Name of the artist.\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"\"\"\n",
    "        SELECT Album.Title, Artist.Name \n",
    "        FROM Album \n",
    "        INNER JOIN Artist \n",
    "        ON Album.ArtistId = Artist.ArtistId\n",
    "    \"\"\")\n",
    "    \n",
    "    df = pd.DataFrame(rs.fetchall())  # Fetch all records into a DataFrame\n",
    "    df.columns = rs.keys()  # Set column names\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "📝 Explanation\n",
    "Execute the SQL query with INNER JOIN:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "SELECT Album.Title, Artist.Name \n",
    "FROM Album \n",
    "INNER JOIN Artist \n",
    "ON Album.ArtistId = Artist.ArtistId\n",
    "Selects Title from the Album table.\n",
    "Selects Name from the Artist table.\n",
    "INNER JOIN ensures only matching ArtistId values from both tables are included.\n",
    "Fetch the query result into a DataFrame:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "This stores the result as a Pandas DataFrame.\n",
    "Set column names:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.columns = rs.keys()\n",
    "Assigns the correct column names from the SQL query.\n",
    "Print the first few records:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.head())\n",
    "Displays the top results.\n",
    "🎯 Key Takeaways\n",
    "✅ INNER JOIN combines records from two tables based on a common column.\n",
    "✅ The ArtistId column is the primary key in Artist and a foreign key in Album.\n",
    "✅ rs.fetchall() retrieves all the results, and rs.keys() gets column names.\n",
    "\n",
    "Let me know if you need further clarification! 🚀😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Filtering your INNER JOIN\n",
    "Congrats on performing your first INNER JOIN! You're now going to finish this chapter with one final exercise in which you perform an INNER JOIN and filter the result using a WHERE clause.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "Instructions\n",
    "100 XP\n",
    "Use the pandas function read_sql_query() to assign to the variable df the DataFrame of results from the following query: select all records from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId that satisfy the condition Milliseconds < 250000.\n",
    "\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's the solution using read_sql_query() to perform an INNER JOIN while filtering records with WHERE Milliseconds < 250000:\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT * \n",
    "    FROM PlaylistTrack \n",
    "    INNER JOIN Track \n",
    "    ON PlaylistTrack.TrackId = Track.TrackId\n",
    "    WHERE Milliseconds < 250000\n",
    "\"\"\", engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "📝 Explanation\n",
    "Perform INNER JOIN on PlaylistTrack and Track:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "FROM PlaylistTrack \n",
    "INNER JOIN Track \n",
    "ON PlaylistTrack.TrackId = Track.TrackId\n",
    "Merges records from both tables where TrackId matches.\n",
    "Filter the result:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "WHERE Milliseconds < 250000\n",
    "Ensures only tracks with a duration of less than 250,000 milliseconds (4 minutes, 10 seconds) are included.\n",
    "Use read_sql_query():\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_sql_query(\"SQL QUERY\", engine)\n",
    "This directly loads SQL query results into a Pandas DataFrame.\n",
    "Print the first few records:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.head())\n",
    "Displays the top rows for verification.\n",
    "🎯 Key Takeaways\n",
    "✅ INNER JOIN allows us to combine records based on a common column (TrackId).\n",
    "✅ WHERE Milliseconds < 250000 filters short-duration tracks.\n",
    "✅ read_sql_query() simplifies SQL execution and conversion into Pandas DataFrame.\n",
    "\n",
    "Let me know if you need further clarification! 🚀😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "The power of SQL lies in relationships between tables: INNER JOIN\n",
    "Here, you'll perform your first INNER JOIN! You'll be working with your favourite SQLite database, Chinook.sqlite. For each record in the Album table, you'll extract the Title along with the Name of the Artist. The latter will come from the Artist table and so you will need to INNER JOIN these two tables on the ArtistID column of both.\n",
    "\n",
    "Recall that to INNER JOIN the Orders and Customers tables from the Northwind database, Hugo executed the following SQL query:\n",
    "\n",
    "\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "The following code has already been executed to import the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "Instructions\n",
    "100 XP\n",
    "Assign to rs the results from the following query: select all the records, extracting the Title of the record and Name of the artist of each record from the Album table and the Artist table, respectively. To do so, INNER JOIN these two tables on the ArtistID column of both.\n",
    "In a call to pd.DataFrame(), apply the method fetchall() to rs in order to fetch all records in rs. Store them in the DataFrame df.\n",
    "Set the DataFrame's column names to the corresponding names of the table columns.\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    ____\n",
    "    ____\n",
    "    ____\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "ChatGPT said:\n",
    "Here's the solution for performing an INNER JOIN between the Album and Artist tables to extract Title (Album) and Name (Artist):\n",
    "\n",
    "✅ Solution\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    # Execute INNER JOIN query\n",
    "    rs = con.execute(\"\"\"\n",
    "        SELECT Album.Title, Artist.Name\n",
    "        FROM Album\n",
    "        INNER JOIN Artist\n",
    "        ON Album.ArtistId = Artist.ArtistId\n",
    "    \"\"\")\n",
    "\n",
    "    # Convert result set into a Pandas DataFrame\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set column names\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "📝 Explanation\n",
    "Perform INNER JOIN on Album and Artist:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "FROM Album\n",
    "INNER JOIN Artist\n",
    "ON Album.ArtistId = Artist.ArtistId\n",
    "Merges records where ArtistId matches in both tables.\n",
    "Select specific columns:\n",
    "\n",
    "sql\n",
    "Copy\n",
    "Edit\n",
    "SELECT Album.Title, Artist.Name\n",
    "Retrieves the album title (Album.Title) and artist name (Artist.Name).\n",
    "Fetch and convert results:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys()\n",
    "fetchall() retrieves all matching records.\n",
    "rs.keys() assigns proper column names to the DataFrame.\n",
    "Print the first few records:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.head())\n",
    "Displays the top rows for verification.\n",
    "🎯 Key Takeaways\n",
    "✅ INNER JOIN allows combining data based on a common column (ArtistId).\n",
    "✅ Selecting specific columns (Title, Name) keeps results clean.\n",
    "✅ Using Pandas DataFrame (pd.DataFrame(rs.fetchall())) makes it easy to manipulate data.\n",
    "\n",
    "Let me know if you need any clarifications! 🚀😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World of SQL Queries! Now, it's time for liftoff! In this exercise, you'll perform the Hello World of SQL queries, SELECT, in order to retrieve all columns of the table Album in the Chinook database. Recall that the query SELECT * selects all columns. Instructions 100 XP Open the engine connection as con using the method .connect() on the engine. Execute the query that selects ALL columns from the Album table. Store the results in rs. Store all of your query results in the DataFrame df by applying the .fetchall() method to the results rs. Close the connection! # Import packages from sqlalchemy import create_engine import pandas as pd # Create engine: engine engine = create_engine('sqlite:///Chinook.sqlite') # Open engine connection: con # Perform query: rs rs = con.execute(____) # Save results of the query to DataFrame: df df = pd.DataFrame(____) # Close connection # Print head of DataFrame df print(df.head())\n",
    "Sure, let’s fill in the blanks together:\n",
    "\n",
    "python\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con = engine.connect()\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "Here’s a breakdown of each step:\n",
    "\n",
    "Create an engine for the Chinook SQLite database.\n",
    "\n",
    "Open a connection to the database.\n",
    "\n",
    "Execute the SQL query to select all columns from the Album table.\n",
    "\n",
    "Store the results in a DataFrame.\n",
    "\n",
    "Close the connection to the database.\n",
    "\n",
    "Print the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth about salaries\n",
    "Your predictive model for customer churn, which you worked on in Chapter 1, has been deployed. Your project manager asks you to work on a new internal project. The goal is to analyze a database with employee salaries in San Francisco, USA.\n",
    "\n",
    "After doing an exhaustive exploratory data analysis, you have to present your findings to the human resources team. They want to compare San Francisco salary growth to the one at the company; they need to understand how to forecast salaries for the next year. You are about to copy the graphs from your analysis. Your manager reminds you to select the right data for your stakeholders.\n",
    "\n",
    "You start by writing down what you believe can help you choose the proper findings.\n",
    "\n",
    "One of the statements you wrote is false. Can you select which one it is?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "The human resource team would likely be interested in knowing how the average salary has been increasing in the last 10 years in San Francisco.\n",
    "Press\n",
    "1\n",
    "\n",
    "The human resource team has no knowledge of data analysis techniques, so code shouldn't be included when listing the top 5 job titles.\n",
    "Press\n",
    "2\n",
    "\n",
    "Select categorical data, such as the salaries on the top 10 rated companies in industry the company evolves in, that provides context to support the idea of the increased salaries.\n",
    "Press\n",
    "3\n",
    "\n",
    "Select all collected numerical data about San Francisco salaries and show them in a big dashboard so it helps understand in detail why salaries have been increasing.\n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "The false statement is:\n",
    "\n",
    "➡️ \"Select all collected numerical data about San Francisco salaries and show them in a big dashboard so it helps understand in detail why salaries have been increasing.\" (Option 4)\n",
    "\n",
    "Explanation:\n",
    "A big dashboard with all numerical data might be overwhelming and not necessarily useful for stakeholders.\n",
    "HR teams need actionable insights, not raw data. Instead of dumping all numbers, you should focus on key trends and comparisons that align with their goals.\n",
    "Better Approach: Present summary statistics, trends, and visualizations to make the data more digestible and meaningful.\n",
    "✅ Correct Statements:\n",
    "1️⃣ Average salary trends over the past 10 years are useful to HR.\n",
    "2️⃣ HR may not understand code, so job titles should be presented in a clear format.\n",
    "3️⃣ Categorical data, like top companies’ salaries, provides useful context.\n",
    "\n",
    "Would you like help with analyzing salary trends using Python? 😊\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "You said:\n",
    "\n",
    "ChatGPT said:\n",
    "The best visualization to use depends on the message you want to convey.\n",
    "\n",
    "If you want to clearly show whether salaries increased or decreased by job title, the right-side chart (Salary change % 2017-2018) is the better choice.\n",
    "\n",
    "Why?\n",
    "✅ The percentage change chart (right) directly shows whether salaries increased or decreased for each role.\n",
    "✅ It makes it easy to see that Project Managers' salaries decreased, while Researchers and Lawyers saw increases.\n",
    "✅ It focuses on relative changes, which is useful for HR when comparing trends.\n",
    "\n",
    "Why not the left chart (Total Pay)?\n",
    "🚫 The total pay chart (left) shows salary values but doesn't clearly highlight salary growth or decline.\n",
    "🚫 It's harder to compare changes at a glance.\n",
    "\n",
    "So, the right-side chart is the best choice for presenting salary variations to HR. 🎯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "ChatGPT said:\n",
    "The correct answer is Option 3:\n",
    "\n",
    "✅ \"The graph on the right is the best way to convey the message. With percentage change, the magnitude of the salary change depending on the job is more evident.\"\n",
    "\n",
    "Why?\n",
    "The percentage change chart (right) is better at highlighting whether salaries increased or decreased.\n",
    "It clearly shows trends:\n",
    "Researchers had a salary increase\n",
    "Project Managers had a salary decrease\n",
    "Lawyers had a small salary increase\n",
    "HR is interested in trends, not just raw salary values.\n",
    "Why not Option 1 or 2?\n",
    "❌ Option 1: While both graphs provide insights, the right-side chart is more effective for conveying salary trends.\n",
    "❌ Option 2: The total pay chart (left) shows raw values but doesn’t clearly communicate percentage increases or decreases.\n",
    "\n",
    "Thus, Option 3 is the best choice! 🎯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "A communication problem\n",
    "Your coworker has been working on a project on price predictions. He asks you to help him choose the most suitable format to deliver his results to the executive board as well as to his team.\n",
    "\n",
    "You give him a set of advice and rules of thumb, so he can make an informed decision. When you arrive home, you realize that you made one mistake.\n",
    "\n",
    "Which of the following advice should you not have provided?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "The amount of time the CEO can dedicate to getting up to speed with your analysis is an important factor in your choice of delivery format.\n",
    "Press\n",
    "1\n",
    "\n",
    "If a software engineer in your team wants to continue your project with new data, the central piece of information to include in your meeting is the project conclusions.\n",
    "Press\n",
    "2\n",
    "\n",
    "If your project manager, located in a different time zone, needs your results to communicate them to customers, a written report would be ideal.\n",
    "Press\n",
    "3\n",
    "ChatGPT said:\n",
    "The incorrect advice is:\n",
    "\n",
    "✅ Option 2:\n",
    "\"If a software engineer in your team wants to continue your project with new data, the central piece of information to include in your meeting is the project conclusions.\"\n",
    "\n",
    "Why is this wrong?\n",
    "Software engineers need technical details, not just conclusions.\n",
    "They require code, data sources, methodologies, and assumptions to extend the project effectively.\n",
    "The conclusions alone don’t provide enough information for further development.\n",
    "Why are the other options correct?\n",
    "✔ Option 1: The CEO's time constraints are important when choosing how to present insights. A brief summary or visual dashboard may be more effective than a detailed technical report.\n",
    "\n",
    "✔ Option 3: A written report is best for a project manager in a different time zone, as it allows asynchronous communication and provides a referenceable document.\n",
    "\n",
    "Thus, Option 2 is incorrect because it lacks essential technical details for the software engineer. 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Credit me\n",
    "You are about to leave the office when you get a call from the operation director. She tells you that you need to write a report on the credit score to present to the advisory board. She explains: \"They want to understand your analysis to help plan a strategy to pre-select customers for loans\".\n",
    "\n",
    "Which of the following reports is the most suitable to write in this case?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "An analytic report including a heatmap with correlation between all predictive variables, technical details about your random forest model, and final recommendations.\n",
    "Press\n",
    "1\n",
    "\n",
    "A short report showing a barplot with the financial risks involved if loans are given to defaulted and not defaulted customers.\n",
    "Press\n",
    "2\n",
    "\n",
    "An analytic report with boxplots displaying the relationship between loan and customer traits, and a barplot of most important predictors.\n",
    "Press\n",
    "3\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 3:\n",
    "\"An analytic report with boxplots displaying the relationship between loan and customer traits, and a barplot of most important predictors.\"\n",
    "\n",
    "Why?\n",
    "The advisory board needs insights for strategic planning, not deep technical details.\n",
    "Boxplots help visualize how different customer traits (e.g., income, credit history) affect loan approvals.\n",
    "A barplot of important predictors makes it clear which factors influence credit scores the most, guiding decision-making.\n",
    "Why are the other options incorrect?\n",
    "❌ Option 1:\n",
    "\n",
    "A heatmap of correlations and technical details about the random forest model are too complex for a strategic meeting.\n",
    "The board doesn’t need to understand every technical aspect of the model.\n",
    "❌ Option 2:\n",
    "\n",
    "A short report with only a barplot on financial risks is too simplistic.\n",
    "While useful, it lacks depth in explaining why certain customers default.\n",
    "Thus, Option 3 is the best choice because it balances analytical depth with clear visual insights for strategic decision-making. 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 3:\n",
    "\"It’s a good idea using the red square to emphasize one of the graphs. It will make the audience focus on what graph you are explaining.\"\n",
    "\n",
    "Why?\n",
    "The red square isn't necessary and might distract the audience rather than focusing their attention effectively.\n",
    "The highlighted graph isn't explicitly being discussed in the text, making the red square's purpose unclear.\n",
    "Instead, using annotations, bold labels, or arrows would be a better way to guide attention.\n",
    "Why are the other options correct?\n",
    "✅ Option 1:\n",
    "\n",
    "The headline font choice is a stylistic preference and doesn’t affect readability significantly.\n",
    "✅ Option 2:\n",
    "\n",
    "The number of graphs displayed is adequate; both graphs provide meaningful comparisons without cluttering the slide.\n",
    "Thus, Option 3 is incorrect, making it the best answer! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "The story begins\n",
    "You recently started working as a data scientist at a company named Communicatb. For your first project, you and your team need to analyze customer churn data for a cell phone company. The goal is to predict their behavior and help develop a program to retain customers.\n",
    "\n",
    "Your team lead knows you are an expert on storytelling. She asks you to explain to the team why crafting a compelling story is important when delivering results. You write down a list of reasons to be prepared.\n",
    "\n",
    "One of the statements you wrote is false. Can you select which one is it?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "It will be easier for the audience to remember an anecdote on why customers churn than the correlation coefficients between customer traits.\n",
    "Press\n",
    "1\n",
    "\n",
    "Your findings will be better aligned with change-adverse stakeholder expectations. They will be most likely to implement the program to retain customers.\n",
    "Press\n",
    "2\n",
    "\n",
    "Even if your data do not reveal a distinct customer behavior, storytelling might influence stakeholders to create the retention program.\n",
    "Press\n",
    "3\n",
    "\n",
    "The marketing team will have a better understanding of the impact of your model. It is central since they are creating the retention program.\n",
    "Press\n",
    "4\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 3:\n",
    "\"Even if your data do not reveal a distinct customer behavior, storytelling might influence stakeholders to create the retention program.\"\n",
    "\n",
    "Why?\n",
    "Storytelling should not manipulate or mislead stakeholders. If the data do not reveal a clear pattern, forcing a story to push for a retention program is unethical.\n",
    "Decision-making should be based on data-driven insights, not just persuasive storytelling.\n",
    "While storytelling helps in communication, it cannot replace factual findings from data analysis.\n",
    "Why are the other options correct?\n",
    "✅ Option 1:\n",
    "\n",
    "Anecdotes are more memorable than numbers. People relate more to stories than raw statistics.\n",
    "✅ Option 2:\n",
    "\n",
    "Storytelling helps align findings with stakeholder concerns. It makes change-resistant individuals more open to new ideas.\n",
    "✅ Option 4:\n",
    "\n",
    "The marketing team benefits from clear communication. Understanding your model’s impact helps them build effective retention strategies.\n",
    "Thus, Option 3 is false, making it the best answer! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Is it a true story?\n",
    "You have done an amazing job explaining your exploratory data analysis on the churn project. Now, it's time to run the model to predict customer churn. You know that you will have to craft an effective story to present these results.\n",
    "\n",
    "You want to be prepared. So you read your notes on how to build a compelling narrative. But you realized that one of your notes is not accurate.\n",
    "\n",
    "Which of the following statements is false?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "A compelling narrative is key to presenting relevant insights to your target audience in a meaningful and impactful way.\n",
    "Press\n",
    "1\n",
    "\n",
    "Because you should shape the narrative to your target audience, showing only key points or findings is a good practice.\n",
    "Press\n",
    "2\n",
    "\n",
    "Unless you have a great data groundwork to support your central insight, your findings will need a well-formed and compelling narrative to drive action and change.\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 3:\n",
    "\"Unless you have a great data groundwork to support your central insight, your findings will need a well-formed and compelling narrative to drive action and change.\"\n",
    "\n",
    "Why is this statement false?\n",
    "A compelling narrative cannot replace strong data. If the data do not support a conclusion, no amount of storytelling should be used to mislead or force action.\n",
    "The foundation of any data-driven decision is solid analysis, not just a good story. A story should enhance insights, not compensate for weak or missing data.\n",
    "While storytelling is important, it should always be truthful, transparent, and backed by evidence.\n",
    "Why are the other options correct?\n",
    "✅ Option 1:\n",
    "\n",
    "A strong narrative makes insights impactful and meaningful. Good storytelling helps the audience understand and remember key findings.\n",
    "✅ Option 2:\n",
    "\n",
    "Tailoring the story to your audience is a best practice. Executives may need high-level insights, while data scientists may require technical details.\n",
    "Thus, Option 3 is false, making it the best answer! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 2: alpha\n",
    "\n",
    "Explanation:\n",
    "In Matplotlib, the alpha argument is used to set the transparency level of a plot layer.\n",
    "The value of alpha ranges from 0 (completely transparent) to 1 (completely opaque).\n",
    "In the provided code, setting alpha=0.7 ensures that overlapping areas are partially visible, making it easier to compare distributions.\n",
    "Corrected Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "churn[churn[\"exited\"]==\"Yes\"][\"credit_score\"].hist(alpha=0.7)  \n",
    "churn[churn[\"exited\"]==\"No\"][\"credit_score\"].hist(alpha=0.7)\n",
    "\n",
    "plt.show()\n",
    "Why are the other options incorrect?\n",
    "❌ Option 1 (bins)\n",
    "\n",
    "The bins argument controls the number of bins in the histogram, not transparency.\n",
    "❌ Option 3 (legend)\n",
    "\n",
    "The legend argument adds labels to differentiate between plots but does not affect transparency.\n",
    "Thus, the correct answer is alpha (Option 2)! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Which of the following statements about flat files is incorrect?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "Flat files consist of rows and each row is called a record.\n",
    "Press\n",
    "1\n",
    "\n",
    "Flat files consist of multiple tables with structured relationships between the tables.\n",
    "Press\n",
    "2\n",
    "\n",
    "A record in a flat file is composed of fields or attributes, each of which contains at most one item of information.\n",
    "Press\n",
    "3\n",
    "\n",
    "Flat files are pervasive in data science.\n",
    "Press\n",
    "4\n",
    "ChatGPT said:\n",
    "Correct Answer:\n",
    "✅ Option 2: \"Flat files consist of multiple tables with structured relationships between the tables.\"\n",
    "\n",
    "Explanation:\n",
    "A flat file is a simple data storage format where data is stored in a plain text or binary file without structured relationships between tables.\n",
    "\n",
    "Why is Option 2 incorrect?\n",
    "Flat files do not support multiple tables or relationships like relational databases (e.g., SQL).\n",
    "They store data in a single table-like structure where each row represents a record and each column represents an attribute or field.\n",
    "Why are the other options correct?\n",
    "✅ Option 1: Flat files consist of rows, and each row is called a record.\n",
    "✔️ True – Each row in a flat file represents a single record of data.\n",
    "\n",
    "✅ Option 3: A record in a flat file is composed of fields or attributes, each of which contains at most one item of information.\n",
    "✔️ True – Each record has fields (or attributes), typically separated by commas (CSV), tabs (TSV), or other delimiters.\n",
    "\n",
    "✅ Option 4: Flat files are pervasive in data science.\n",
    "✔️ True – Data scientists frequently use flat files (CSV, TXT, JSON) to store and process large datasets.\n",
    "\n",
    "Final Answer:\n",
    "❌ Option 2 is incorrect because flat files do not support multiple tables with structured relationships.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Fill in the arguments of np.loadtxt() by passing file and a comma ',' for the delimiter.\n",
    "Fill in the argument of print() to print the type of the object digits. Use the function type().\n",
    "Execute the rest of the code to visualize one of the rows of the data.\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "# Assign filename to variable: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load file as array: digits\n",
    "digits = np.loadtxt(____, delimiter='____')\n",
    "\n",
    "# Print datatype of digits\n",
    "print(____)\n",
    "\n",
    "# Select and reshape a row\n",
    "im = digits[21, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data (matplotlib.pyplot already loaded as plt)\n",
    "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "ChatGPT said:\n",
    "Here’s the corrected version of the code with the missing arguments filled in:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Ensure matplotlib is imported\n",
    "\n",
    "# Assign filename to variable: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load file as array: digits\n",
    "digits = np.loadtxt(file, delimiter=',')\n",
    "\n",
    "# Print datatype of digits\n",
    "print(type(digits))\n",
    "\n",
    "# Select and reshape a row\n",
    "im = digits[21, 1:]\n",
    "im_sq = np.reshape(im, (28, 28))\n",
    "\n",
    "# Plot reshaped data\n",
    "plt.imshow(im_sq, cmap='Greys', interpolation='nearest')\n",
    "plt.show()\n",
    "Explanation of the Fixes:\n",
    "file as the first argument in np.loadtxt()\n",
    "\n",
    "The function np.loadtxt() needs the filename passed as a string. Since file is already assigned to 'digits.csv', we use file as the argument.\n",
    "delimiter=','\n",
    "\n",
    "The file is assumed to be a CSV (comma-separated values) file, so the delimiter must be a comma (',').\n",
    "print(type(digits))\n",
    "\n",
    "This prints the type of digits, which should be <class 'numpy.ndarray'>.\n",
    "Ensuring matplotlib.pyplot is imported\n",
    "\n",
    "The code assumes plt is already loaded, but we explicitly import it for clarity.\n",
    "Expected Output:\n",
    "The output of print(type(digits)) should be:\n",
    "javascript\n",
    "Copy\n",
    "Edit\n",
    "<class 'numpy.ndarray'>\n",
    "The code will display a 28x28 grayscale image, which is a reshaped row from the digits dataset.\n",
    "Let me know if you need further clarification! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how do i print all the content in digits.csv file\n",
    "ChatGPT said:\n",
    "You can print all the contents of digits.csv using Python in several ways. Here are some approaches:\n",
    "\n",
    "1. Using np.loadtxt()\n",
    "If the file is purely numerical (no headers or mixed data types), you can use numpy.loadtxt():\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import numpy as np\n",
    "\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load the entire CSV file\n",
    "digits = np.loadtxt(file, delimiter=',')\n",
    "\n",
    "# Print the entire content\n",
    "print(digits)\n",
    "2. Using pandas for Better Formatting\n",
    "If the CSV contains headers or mixed data types, it's better to use pandas.read_csv():\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Print the entire DataFrame\n",
    "print(df)\n",
    "🔹 Advantage: This method properly handles headers, missing values, and text-based data.\n",
    "\n",
    "3. Using open() to Read the File Line by Line\n",
    "If you want to manually read the file as plain text:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Open and read the file\n",
    "with open(file, 'r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# Print each line\n",
    "for line in content:\n",
    "    print(line.strip())  # Strip removes trailing newline characters\n",
    "Which One Should You Use?\n",
    "Use np.loadtxt() if the file contains only numbers.\n",
    "Use pandas.read_csv() if the file contains headers or a mix of text and numbers.\n",
    "Use open() if you just want to see the raw text content.\n",
    "Let me know if you need more details! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(____, delimiter='____', skiprows=____, usecols=[____])\n",
    "\n",
    "# Print data\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "You need to correctly fill in the arguments for np.loadtxt() to load data from digits_header.txt, considering that:\n",
    "\n",
    "The file has headers that should be skipped.\n",
    "The values are separated by a delimiter (probably a comma ,).\n",
    "You want to select specific columns using usecols.\n",
    "Corrected Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data (assuming a comma ',' as delimiter, skipping the first row, and selecting specific columns)\n",
    "data = np.loadtxt(file, delimiter=',', skiprows=1, usecols=[0, 1, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n",
    "Explanation of Arguments:\n",
    "delimiter=',' → Assumes that values in the file are separated by commas.\n",
    "skiprows=1 → Skips the first row, which likely contains column headers.\n",
    "usecols=[0, 1, 2] → Loads only columns 0, 1, and 2 (adjust this based on your needs).\n",
    "If your file uses tabs (\\t) instead of commas, change delimiter=',' to delimiter='\\t'.\n",
    "\n",
    "Need more customization? Let me know! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "here are a number of arguments that np.loadtxt() takes that you'll find useful:\n",
    "\n",
    "delimiter changes the delimiter that loadtxt() is expecting.\n",
    "You can use ',' for comma-delimited.\n",
    "You can use '\\t' for tab-delimited.\n",
    "skiprows allows you to specify how many rows (not indices) you wish to skip.\n",
    "usecols takes a list of the indices of the columns you wish to keep.\n",
    "The file that you'll be importing, digits_header.txt, has a header and is tab-delimited.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the arguments of np.loadtxt(): the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.\n",
    "Complete the argument of the print() call in order to print the entire array that you just imported.\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data: data\n",
    "data = np.loadtxt(____, delimiter='____', skiprows=____, usecols=[____])\n",
    "\n",
    "# Print data\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Corrected Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits_header.txt'\n",
    "\n",
    "# Load the data (tab-delimited, skip first row, select first and third columns)\n",
    "data = np.loadtxt(file, delimiter='\\t', skiprows=1, usecols=[0, 2])\n",
    "\n",
    "# Print data\n",
    "print(data)\n",
    "Explanation of Arguments:\n",
    "delimiter='\\t' → The file is tab-delimited.\n",
    "skiprows=1 → Skips the first row (header).\n",
    "usecols=[0, 2] → Loads only the first (index 0) and third (index 2) columns.\n",
    "This will output the filtered numerical data from digits_header.txt. 🚀 Let me know if you need more adjustments!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how do I view header.txt using pandas \n",
    "ChatGPT said:\n",
    "To view the header.txt file using pandas, follow these steps:\n",
    "\n",
    "Code to Load and View the File:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file into a pandas DataFrame\n",
    "df = pd.read_csv(\"header.txt\", delimiter='\\t')  # Use '\\t' for tab-separated files\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())  # Shows the first 5 rows\n",
    "Explanation:\n",
    "pd.read_csv(\"header.txt\", delimiter='\\t') → Reads the tab-separated file into a pandas DataFrame.\n",
    "df.head() → Displays the first five rows of the DataFrame.\n",
    "🔹 If your file uses a different delimiter (e.g., commas , instead of tabs \\t), adjust the delimiter argument:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_csv(\"header.txt\", delimiter=',')  # Use ',' for comma-separated files\n",
    "Let me know if you need further help! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Importing different datatypes\n",
    "The file seaslug.txt\n",
    "\n",
    "has a text header, consisting of strings\n",
    "is tab-delimited.\n",
    "This data consists of the percentage of sea slug larvae that had metamorphosed in a given time period. Read more here.\n",
    "\n",
    "Due to the header, if you tried to import it as-is using np.loadtxt(), Python would throw you a ValueError and tell you that it could not convert string to float. There are two ways to deal with this: firstly, you can set the data type argument dtype equal to str (for string).\n",
    "\n",
    "Alternatively, you can skip the first row as we have seen before, using the skiprows argument.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the first call to np.loadtxt() by passing file as the first argument.\n",
    "Execute print(data[0]) to print the first element of data.\n",
    "Complete the second call to np.loadtxt(). The file you're importing is tab-delimited, the datatype is float, and you want to skip the first row.\n",
    "Print the 10th element of data_float by completing the print() command. Be guided by the previous print() call.\n",
    "Execute the rest of the code to visualize the data.\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = np.loadtxt(____, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import file as floats and skip the first row: data_float\n",
    "data_float = np.loadtxt(____, delimiter='____', dtype=____, skiprows=____)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(____)\n",
    "\n",
    "# Plot a scatterplot of the data\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()\n",
    "ChatGPT said:\n",
    "Here’s how you can complete the code to correctly import the seaslug.txt file while handling different data types:\n",
    "\n",
    "Complete Code:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename\n",
    "file = 'seaslug.txt'\n",
    "\n",
    "# Import file as string data\n",
    "data = np.loadtxt(file, delimiter='\\t', dtype=str)\n",
    "\n",
    "# Print the first element of data\n",
    "print(data[0])\n",
    "\n",
    "# Import file as float and skip the first row\n",
    "data_float = np.loadtxt(file, delimiter='\\t', dtype=float, skiprows=1)\n",
    "\n",
    "# Print the 10th element of data_float\n",
    "print(data_float[9])  # Indexing starts at 0, so the 10th element is at index 9\n",
    "\n",
    "# Plot scatter plot\n",
    "plt.scatter(data_float[:, 0], data_float[:, 1])\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('percentage of larvae')\n",
    "plt.show()\n",
    "Explanation:\n",
    "First np.loadtxt() call:\n",
    "\n",
    "Loads the entire file as strings (dtype=str).\n",
    "This helps us examine the header or mixed data before conversion.\n",
    "Second np.loadtxt() call:\n",
    "\n",
    "Skips the first row (skiprows=1) because it contains the header (which is non-numeric).\n",
    "Reads the remaining data as float values (dtype=float).\n",
    "Printing the 10th element:\n",
    "\n",
    "Since Python uses zero-based indexing, the 10th element is at index 9 (data_float[9]).\n",
    "Scatter Plot:\n",
    "\n",
    "The first column (data_float[:, 0]) represents time (minutes).\n",
    "The second column (data_float[:, 1]) represents percentage of larvae metamorphosed.\n",
    "✅ Key Takeaways:\n",
    "dtype=str → Handles mixed data or headers.\n",
    "skiprows=1 → Skips non-numeric header rows.\n",
    "delimiter='\\t' → Specifies tab-separated data.\n",
    "Indexing starts from 0 → So, the 10th element is data_float[9].\n",
    "This approach ensures smooth data handling and visualization. 🚀 Let me know if you need any clarifications! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how do i display the contents of seaslug.txt not importing but I want to see what is in the raw file\n",
    "ChatGPT said:\n",
    "To display the raw contents of seaslug.txt without importing it as structured data, you can use Python's built-in file handling functions. Here are a few ways to do it:\n",
    "\n",
    "Method 1: Using open() and read()\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open and read the entire file\n",
    "with open('seaslug.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    \n",
    "# Print the raw contents\n",
    "print(contents)\n",
    "Method 2: Using open() and readlines() (Line by Line)\n",
    "If the file is large, reading it line by line is more efficient:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open file and read line by line\n",
    "with open('seaslug.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())  # strip() removes unnecessary whitespace/newlines\n",
    "Method 3: Using !cat (For Jupyter Notebook or Unix-based systems)\n",
    "If you are using Jupyter Notebook or a Unix-based system (Linux/macOS), you can run:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "!cat seaslug.txt\n",
    "or on Windows (PowerShell):\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "!type seaslug.txt\n",
    "Output Example (Raw File Preview)\n",
    "python-repl\n",
    "Copy\n",
    "Edit\n",
    "Time (min.)\tPercentage of larvae\n",
    "0\t0.0\n",
    "5\t0.2\n",
    "10\t0.5\n",
    "15\t0.7\n",
    "...\n",
    "This will show the exact content of the file, including headers, delimiters, and raw formatting.\n",
    "\n",
    "Let me know if you need further clarification! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how do I view everything in digits.text, head_digits.txt\n",
    "ChatGPT said:\n",
    "To view the raw contents of digits.txt or head_digits.txt, you can use the following methods in Python.\n",
    "\n",
    "Method 1: Using open() and read()\n",
    "This reads the entire file and prints it as a string.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open and read the entire file\n",
    "with open('digits.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "# Print the raw contents\n",
    "print(contents)\n",
    "Method 2: Using open() and readlines()\n",
    "If the file is large, reading it line by line is more efficient:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open file and read line by line\n",
    "with open('digits.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())  # strip() removes unnecessary whitespace/newlines\n",
    "Method 3: Using Pandas (pd.read_csv())\n",
    "If the file is structured (e.g., CSV, tab-delimited, space-delimited):\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load and display the entire file\n",
    "df = pd.read_csv('digits.txt', delimiter='\\t')  # Change delimiter if needed\n",
    "print(df)\n",
    "Method 4: Using Shell Commands (For Jupyter Notebook or Terminal)\n",
    "If using Jupyter Notebook, Linux, or macOS, run:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "!cat digits.txt  # Show entire file\n",
    "For Windows (PowerShell):\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "!type digits.txt\n",
    "For Windows (Command Prompt):\n",
    "\n",
    "cmd\n",
    "Copy\n",
    "Edit\n",
    "more digits.txt\n",
    "Bonus: View Only First Few Lines\n",
    "To check just the first 5 lines of a large file:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with open('digits.txt', 'r') as file:\n",
    "    for i in range(5):\n",
    "        print(file.readline().strip())\n",
    "Let me know if you need more help! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Using pandas to import flat files as DataFrames (1)\n",
    "In the last exercise, you were able to import flat files containing columns with different datatypes as numpy arrays. However, the DataFrame object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files of mixed data types as DataFrames using the pandas functions read_csv() and read_table().\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pandas package using the alias pd.\n",
    "Read titanic.csv into a DataFrame called df. The file name is already stored in the file object.\n",
    "In a print() call, view the head of the DataFrame.\n",
    "\n",
    "# Import pandas as pd\n",
    "____\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv(____)\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here’s the complete solution to your task:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'titanic.csv'\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# View the head of the DataFrame\n",
    "print(df.head())\n",
    "Explanation:\n",
    "Import pandas using the alias pd.\n",
    "Read the CSV file titanic.csv into a pandas DataFrame using pd.read_csv(file).\n",
    "Print the first few rows using .head() to check the structure of the dataset.\n",
    "Let me know if you need any modifications! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Not so flat any more\n",
    "You can explore your current working directory natively in Python using the library os, which consists of miscellaneous operating system interfaces.\n",
    "\n",
    "The first line of the following code imports the library os, the second line stores the name of the current directory in a string called wd and the third outputs the contents of the directory in a list to the shell.\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)\n",
    "Run this code in the shell and answer the following questions. Ignore the files that begin with ..\n",
    "\n",
    "Check out the contents of your current directory and answer the following questions: (1) which file is in your directory and NOT an example of a flat file; (2) why is it not a flat file?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "database.db is not a flat file because relational databases contain structured relationships and flat files do not.\n",
    "\n",
    "battledeath.xlsx is not a flat because it is a spreadsheet consisting of many sheets, not a single table.\n",
    "\n",
    "titanic.txt is not a flat file because it is a .txt, not a .csv.\n",
    "ChatGPT said:\n",
    "The correct answer is:\n",
    "\n",
    "database.db is not a flat file because relational databases contain structured relationships and flat files do not.\n",
    "\n",
    "Explanation:\n",
    "Flat files store data in a simple format (like .csv or .txt) with no structured relationships between records.\n",
    "database.db is a relational database, which means it contains structured relationships between multiple tables. Unlike flat files, it supports complex queries and indexing.\n",
    "battledeath.xlsx is also not a flat file, but the reason given is incorrect because Excel files can contain single-table sheets.\n",
    "titanic.txt is a flat file if it contains tabular data (e.g., comma-separated values), despite being a .txt file.\n",
    "Would you like me to explain further? 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Loading a pickled file\n",
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n",
    "\n",
    "In this exercise, you'll import the pickle package, open a previously pickled data structure from a file and load it.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pickle package.\n",
    "Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'.\n",
    "Pass the correct argument to pickle.load(); it should use the variable that is bound to open.\n",
    "Print the data, d.\n",
    "Print the datatype of d; take your mind back to your previous use of the function type().\n",
    "ChatGPT said:\n",
    "Here's how you can load a pickled file in Python:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import the pickle package\n",
    "import pickle\n",
    "\n",
    "# Open the file in read-binary mode\n",
    "with open('data.pkl', 'rb') as file:  # 'rb' means read binary\n",
    "    # Load the data from the pickle file\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(d)\n",
    "\n",
    "# Print the datatype of d\n",
    "print(type(d))\n",
    "Explanation:\n",
    "import pickle → Imports the pickle module for working with serialized objects.\n",
    "open('data.pkl', 'rb') → Opens the file 'data.pkl' in read-binary ('rb') mode since pickle files are stored in binary format.\n",
    "pickle.load(file) → Reads the pickled data from the file and deserializes it into a Python object.\n",
    "print(d) → Displays the loaded data.\n",
    "print(type(d)) → Shows the data type of d, which could be a list, dictionary, or any other Python object.\n",
    "Would you like to try saving your own objects as a pickle file too? 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Loading a pickled file\n",
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n",
    "\n",
    "In this exercise, you'll import the pickle package, open a previously pickled data structure from a file and load it.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the pickle package.\n",
    "Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'.\n",
    "Pass the correct argument to pickle.load(); it should use the variable that is bound to open.\n",
    "Print the data, d.\n",
    "Print the datatype of d; take your mind back to your previous use of the function type().\n",
    "\n",
    "# Import pickle package\n",
    "\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', ____) as file:\n",
    "    d = pickle.load(____)\n",
    "\n",
    "# Print d\n",
    "print(____)\n",
    "\n",
    "# Print datatype of d\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here's the correct way to complete the code:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:  # 'rb' means read binary\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))\n",
    "Explanation:\n",
    "import pickle → Imports the pickle module for working with serialized objects.\n",
    "open('data.pkl', 'rb') → Opens 'data.pkl' in read-binary ('rb') mode since pickle files are stored in binary format.\n",
    "pickle.load(file) → Reads the pickled data from the file and deserializes it into a Python object.\n",
    "print(d) → Displays the loaded data.\n",
    "print(type(d)) → Shows the data type of d, which could be a list, dictionary, or any other Python object.\n",
    "Let me know if you need more clarification! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Listing sheets in Excel files\n",
    "Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in time. You won't always want to do so in Excel, however!\n",
    "\n",
    "Here, you'll learn how to use pandas to import Excel spreadsheets and how to list the names of the sheets in any loaded .xlsx file.\n",
    "\n",
    "Recall from the video that, given an Excel file imported into a variable spreadsheet, you can retrieve a list of the sheet names using the attribute spreadsheet.sheet_names.\n",
    "\n",
    "Specifically, you'll be loading and checking out the spreadsheet 'battledeath.xlsx', modified from the Peace Research Institute Oslo's (PRIO) dataset. This data contains age-adjusted mortality rates due to war in various countries over several years.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Assign the spreadsheet filename (provided above) to the variable file.\n",
    "Pass the correct argument to pd.ExcelFile() to load the file using pandas, assigning the result to the variable xls.\n",
    "Print the sheetnames of the Excel spreadsheet by passing the necessary argument to the print() function.\n",
    "\n",
    " Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = ____\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(____)\n",
    "\n",
    "# Print sheet names\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here’s the correct way to complete the code:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n",
    "Explanation:\n",
    "import pandas as pd → Imports the pandas library.\n",
    "file = 'battledeath.xlsx' → Stores the Excel file's name as a variable.\n",
    "pd.ExcelFile(file) → Reads the Excel file into a pandas ExcelFile object.\n",
    "xls.sheet_names → Lists all sheet names present in the Excel file.\n",
    "Let me know if you need further clarification! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Importing sheets from Excel files\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Load the sheet '2004' into the DataFrame df1 using its name as a string.\n",
    "Print the head of df1 to the shell.\n",
    "Load the sheet 2002 into the DataFrame df2 using its index (0).\n",
    "Print the head of df2 to the shell.\n",
    "\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse(____)\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(____)\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here's the correct way to complete your code:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n",
    "Explanation:\n",
    "xls.parse('2004') → Reads the sheet named '2004' into df1.\n",
    "df1.head() → Prints the first 5 rows of df1 to verify the data.\n",
    "xls.parse(0) → Reads the sheet at index 0 (which corresponds to '2002') into df2.\n",
    "df2.head() → Prints the first 5 rows of df2.\n",
    "Let me know if you have any questions! 😊\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Importing sheets from Excel files\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, '2002' and '2004'. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Load the sheet '2004' into the DataFrame df1 using its name as a string.\n",
    "Print the head of df1 to the shell.\n",
    "Load the sheet 2002 into the DataFrame df2 using its index (0).\n",
    "Print the head of df2 to the shell.\n",
    "\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse(____)\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(____)\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here's the correct way to complete your code:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n",
    "Explanation:\n",
    "xls.parse('2004') → Loads the sheet named '2004' into df1.\n",
    "df1.head() → Displays the first 5 rows of df1.\n",
    "xls.parse(0) → Loads the sheet at index 0 (which corresponds to '2002') into df2.\n",
    "df2.head() → Displays the first 5 rows of df2.\n",
    "Let me know if you need further clarification! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how will I view the content of 'battledeath.xlsx'\n",
    "ChatGPT said:\n",
    "To view the content of 'battledeath.xlsx', follow these steps using pandas:\n",
    "\n",
    "1. List the Sheet Names\n",
    "Before viewing the content, check which sheets exist in the file:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file = 'battledeath.xlsx'\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print all sheet names\n",
    "print(xls.sheet_names)\n",
    "2. View the Content of a Specific Sheet\n",
    "Once you know the sheet names, you can load and display a specific sheet:\n",
    "\n",
    "Option 1: Load by Sheet Name\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = xls.parse('2004')  # Change '2004' to the sheet name you want to view\n",
    "print(df)\n",
    "Option 2: Load by Sheet Index\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = xls.parse(0)  # Loads the first sheet\n",
    "print(df)\n",
    "3. View First Few Rows\n",
    "If the file is large, use .head() to show only the first few rows:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.head())\n",
    "4. View Summary Statistics\n",
    "For a quick overview:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(df.info())  # Shows column data types and missing values\n",
    "print(df.describe())  # Shows summary statistics for numerical columns\n",
    "Let me know if you need more details! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "as in everything from the raw file\n",
    "ChatGPT said:\n",
    "To view the entire raw content of 'battledeath.xlsx' (all sheets and their contents), use pandas along with openpyxl or xlrd (for .xlsx files).\n",
    "\n",
    "1. Read and Print All Sheets\n",
    "This will load all sheets and display their contents:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file = 'battledeath.xlsx'\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Iterate through all sheets and print their contents\n",
    "for sheet in xls.sheet_names:\n",
    "    df = xls.parse(sheet)\n",
    "    print(f\"Sheet: {sheet}\")\n",
    "    print(df)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator for better readability\n",
    "2. Save Everything to a Text File (Optional)\n",
    "If you want to save the raw content into a text file instead of printing:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with open(\"battledeath_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = xls.parse(sheet)\n",
    "        f.write(f\"Sheet: {sheet}\\n\")\n",
    "        f.write(df.to_string())  # Converts DataFrame to string\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "This will create a file named battledeath_raw.txt with all the raw content.\n",
    "\n",
    "3. Alternative: Convert to CSV\n",
    "To export each sheet as a separate CSV:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "for sheet in xls.sheet_names:\n",
    "    df = xls.parse(sheet)\n",
    "    df.to_csv(f\"{sheet}.csv\", index=False)  # Saves as CSV file\n",
    "This will generate separate .csv files for each sheet.\n",
    "\n",
    "Would you like to see the output formatted differently? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Im seen something like this. is like some content is missing\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator for better readability\n",
    "Sheet: 2002\n",
    "    War, age-adjusted mortality due to    2002\n",
    "0                          Afghanistan  36.084\n",
    "1                              Albania   0.129\n",
    "2                              Algeria  18.314\n",
    "3                              Andorra   0.000\n",
    "4                               Angola  18.965\n",
    "..                                 ...     ...\n",
    "187                          Venezuela   0.000\n",
    "188                            Vietnam   0.040\n",
    "189                        Yemen, Rep.   0.075\n",
    "190                             Zambia   0.045\n",
    "191                           Zimbabwe  33.796\n",
    "\n",
    "[192 rows x 2 columns]\n",
    "\n",
    "==================================================\n",
    "\n",
    "Sheet: 2004\n",
    "    War(country)   2004\n",
    "0    Afghanistan  9.451\n",
    "1        Albania  0.130\n",
    "2        Algeria  3.407\n",
    "3        Andorra  0.000\n",
    "4         Angola  2.598\n",
    "..           ...    ...\n",
    "187    Venezuela  0.000\n",
    "188      Vietnam  0.038\n",
    "189  Yemen, Rep.  3.603\n",
    "190       Zambia  0.042\n",
    "191     Zimbabwe  0.510\n",
    "\n",
    "[192 rows x 2 columns]\n",
    "\n",
    "==================================================\n",
    "ChatGPT said:\n",
    "You're seeing the truncated output because pandas automatically shortens large DataFrames when printing to the console.\n",
    "\n",
    "To view all content fully, try one of these options:\n",
    "\n",
    "1. Temporarily Disable Truncation in Pandas\n",
    "Set display options to show all rows and columns before printing:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file = 'battledeath.xlsx'\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Ensure all rows and columns are shown\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Print all content\n",
    "for sheet in xls.sheet_names:\n",
    "    df = xls.parse(sheet)\n",
    "    print(f\"\\nSheet: {sheet}\\n\")\n",
    "    print(df.to_string())  # Converts DataFrame to full string\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator for readability\n",
    "🔹 Note: This will display everything on your screen, so if the data is large, it might be overwhelming.\n",
    "\n",
    "2. Export Everything to a Text File\n",
    "If the data is too large for your console, save it to a file instead:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with open(\"battledeath_full.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = xls.parse(sheet)\n",
    "        f.write(f\"Sheet: {sheet}\\n\")\n",
    "        f.write(df.to_string())  # Write full DataFrame to file\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Data saved to 'battledeath_full.txt' ✅\")\n",
    "🔹 Now, you can open battledeath_full.txt in a text editor to see everything.\n",
    "\n",
    "3. View in a Pandas GUI (Optional)\n",
    "If you want to see everything in a scrollable table instead of printing:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file = 'battledeath.xlsx'\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Open each sheet in an interactive Pandas GUI\n",
    "for sheet in xls.sheet_names:\n",
    "    df = xls.parse(sheet)\n",
    "    print(f\"Opening sheet: {sheet}\")\n",
    "    df.to_csv(f\"{sheet}.csv\", index=False)  # Convert to CSV for easier viewing\n",
    "    print(f\"Saved {sheet}.csv for viewing.\")\n",
    "You can open the CSVs in Excel or use Pandas GUI tools like df.style in Jupyter Notebook.\n",
    "\n",
    "Which method works best for you? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "what about using python code to view the entire file instead of pandas\n",
    "ChatGPT said:\n",
    "If you want to view the entire raw content of battledeath.xlsx without using pandas, you can use openpyxl (for .xlsx files) or xlrd (for older .xls files).\n",
    "\n",
    "Method 1: Using openpyxl (For .xlsx files)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load the entire Excel file\n",
    "file = \"battledeath.xlsx\"\n",
    "wb = load_workbook(file)\n",
    "\n",
    "# Loop through all sheets and print content\n",
    "for sheet_name in wb.sheetnames:\n",
    "    sheet = wb[sheet_name]\n",
    "    print(f\"\\nSheet: {sheet_name}\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Loop through each row and print values\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        print(row)  # Print as a tuple\n",
    "✅ This prints everything as raw tuples from the spreadsheet.\n",
    "\n",
    "Method 2: Using xlrd (For .xls files)\n",
    "If your file is an older .xls format, use xlrd instead:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import xlrd\n",
    "\n",
    "# Open workbook\n",
    "file = \"battledeath.xls\"\n",
    "wb = xlrd.open_workbook(file)\n",
    "\n",
    "# Loop through sheets\n",
    "for sheet in wb.sheets():\n",
    "    print(f\"\\nSheet: {sheet.name}\\n\" + \"=\" * 50)\n",
    "\n",
    "    # Loop through rows and print values\n",
    "    for row_idx in range(sheet.nrows):\n",
    "        print(sheet.row_values(row_idx))  # Print row as a list\n",
    "Method 3: Print Raw Binary Content (Not Readable, But Raw Data)\n",
    "If you literally want to see the raw binary file content, you can read it as bytes:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with open(\"battledeath.xlsx\", \"rb\") as f:\n",
    "    content = f.read()\n",
    "    print(content)  # Prints unreadable binary content\n",
    "🚨 Not human-readable, but it shows the raw bytes stored in the file.\n",
    "\n",
    "Which Method Works Best?\n",
    "✅ Use openpyxl for modern .xlsx files.\n",
    "✅ Use xlrd for old .xls files.\n",
    "❌ Use binary mode only if debugging file corruption.\n",
    "Would you like to extract and save the data instead of just printing it? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Using File to import HDF5 files\n",
    "The h5py package has been imported in the environment and the file LIGO_data.hdf5 is loaded in the object h5py_file.\n",
    "\n",
    "What is the correct way of using the h5py function, File(), to import the file in h5py_file into an object, h5py_data, for reading only?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "h5py_data = File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, read)\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'read')\n",
    "ChatGPT said:\n",
    "The correct way to open the HDF5 file for reading only using h5py.File() is:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "h5py_data = h5py.File(h5py_file, 'r')\n",
    "Explanation:\n",
    "✅ h5py.File(h5py_file, 'r') → Correct because 'r' specifies read mode.\n",
    "❌ File(h5py_file, 'r') → Incorrect because File() is not a standalone function; it belongs to h5py.\n",
    "❌ h5py.File(h5py_file, read) → Incorrect because read is not a valid argument (should be a string 'r').\n",
    "❌ h5py.File(h5py_file, 'read') → Incorrect because 'read' is not a valid mode (should be 'r').\n",
    "Would you like help exploring the contents of the HDF5 file after loading it? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Using File to import HDF5 files\n",
    "The h5py package has been imported in the environment and the file LIGO_data.hdf5 is loaded in the object h5py_file.\n",
    "\n",
    "What is the correct way of using the h5py function, File(), to import the file in h5py_file into an object, h5py_data, for reading only?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "h5py_data = File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'r')\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, read)\n",
    "\n",
    "h5py_data = h5py.File(h5py_file, 'read')\n",
    "ChatGPT said:\n",
    "The correct answer is:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "h5py_data = h5py.File(h5py_file, 'r')\n",
    "Explanation:\n",
    "✅ h5py.File(h5py_file, 'r') → Correct because h5py.File() is the proper function to open an HDF5 file, and 'r' specifies read mode.\n",
    "❌ File(h5py_file, 'r') → Incorrect because File() is not a standalone function; it must be accessed through h5py.\n",
    "❌ h5py.File(h5py_file, read) → Incorrect because read is not a valid argument (it should be a string 'r').\n",
    "❌ h5py.File(h5py_file, 'read') → Incorrect because 'read' is not a valid mode (valid options are 'r', 'w', 'a', etc.).\n",
    "Would you like help exploring the contents of the HDF5 file after loading it? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Using h5py to import HDF5 files\n",
    "The file 'LIGO_data.hdf5' is already in your working directory. In this exercise, you'll import it using the h5py library. You'll also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see precisely what HDF groups it contains.\n",
    "\n",
    "You can find the LIGO data plus loads of documentation and tutorials here. There is also a great tutorial on Signal Processing with the data here.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package h5py.\n",
    "Assign the name of the file to the variable file.\n",
    "Load the file as read only into the variable data.\n",
    "Print the datatype of data.\n",
    "Print the names of the groups in the HDF5 file 'LIGO_data.hdf5'.\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import ____\n",
    "\n",
    "# Assign filename: file\n",
    "\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(____, ____)\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in ____:\n",
    "    print(____)\n",
    "ChatGPT said:\n",
    "Here's the correct way to import and explore the HDF5 file using h5py:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load file as read-only\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))  # Should output: <class 'h5py._hl.files.File'>\n",
    "\n",
    "# Print the keys (top-level groups) of the file\n",
    "print(\"Keys in the HDF5 file:\")\n",
    "for key in data.keys():\n",
    "    print(key)\n",
    "Explanation:\n",
    "Import h5py → Required for working with HDF5 files.\n",
    "Assign the filename → Store the file path in a variable.\n",
    "Open the file in read mode ('r') → Use h5py.File(file, 'r').\n",
    "Check the datatype → Print type(data) to confirm it's an HDF5 file.\n",
    "Explore the contents → Use data.keys() to list top-level groups.\n",
    "Let me know if you need help extracting data from specific groups! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Extracting data from your HDF5 file\n",
    "In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.\n",
    "\n",
    "To do so, you'll need to first explore the HDF5 group 'strain'.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Assign the HDF5 group data['strain'] to group.\n",
    "In the for loop, print out the keys of the HDF5 group in group.\n",
    "Assign the time series data data['strain']['Strain'] to a NumPy array called strain.\n",
    "Set num_samples equal to 10000, the number of time points we wish to sample.\n",
    "Execute the rest of the code to produce a plot of the time series data in LIGO_data.hdf5.\n",
    "\n",
    "# Get the HDF5 group: group\n",
    "\n",
    "\n",
    "# Check out keys of group\n",
    "for key in ____:\n",
    "    print(____)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()\n",
    "ChatGPT said:\n",
    "Here’s the corrected Python script to extract and plot LIGO strain data from the HDF5 file:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load HDF5 file as read-only\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Get the HDF5 group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of the group\n",
    "print(\"Keys in 'strain' group:\")\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data\n",
    "strain = np.array(group['Strain'])  # Convert to NumPy array\n",
    "\n",
    "# Set number of time points to sample\n",
    "num_samples = 10000  \n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('Strain')\n",
    "plt.title('LIGO Strain Data')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "Explanation:\n",
    "Import Libraries → h5py for HDF5 handling, numpy for arrays, and matplotlib.pyplot for plotting.\n",
    "Open the File → Load LIGO_data.hdf5 in read-only mode.\n",
    "Access the strain Group → data['strain'] contains the time series.\n",
    "Explore Keys → Print all keys inside strain to check data structure.\n",
    "Extract Data → Convert data['strain']['Strain'] into a NumPy array.\n",
    "Set Time Points → Define num_samples = 10000 for plotting.\n",
    "Plot the Data → Display the first 10,000 time points.\n",
    "🚀 Let me know if you need further modifications!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Loading .mat files\n",
    "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package scipy.io.\n",
    "Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().\n",
    "Use the function type() to print the datatype of mat to the shell.\n",
    "\n",
    "# Import package\n",
    "\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here’s the correct Python script to load a MATLAB (.mat) file using scipy.io.loadmat():\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype of mat\n",
    "print(type(mat))\n",
    "Explanation:\n",
    "Import scipy.io → The scipy.io module provides functions for handling .mat files.\n",
    "Load the .mat file → scipy.io.loadmat() reads the MATLAB file into a Python dictionary.\n",
    "Check the Data Type → Printing type(mat) will likely output:\n",
    "javascript\n",
    "Copy\n",
    "Edit\n",
    "<class 'dict'>\n",
    "This means the file contents are stored in a Python dictionary where:\n",
    "Keys → Variable names from the MATLAB file.\n",
    "Values → Corresponding arrays or data structures.\n",
    "Next Steps:\n",
    "To explore the file contents, you can print the keys:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(mat.keys())\n",
    "This will show available variables inside albeck_gene_expression.mat.\n",
    "\n",
    "Let me know if you need further help! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Loading .mat files\n",
    "In this exercise, you'll figure out how to load a MATLAB file using scipy.io.loadmat() and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is in your working directory. This file contains gene expression data from the Albeck Lab at UC Davis.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the package scipy.io.\n",
    "Load the file 'albeck_gene_expression.mat' into the variable mat; do so using the function scipy.io.loadmat().\n",
    "Use the function type() to print the datatype of mat to the shell.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import package\n",
    "\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(____)\n",
    "ChatGPT said:\n",
    "Here's the correct Python script to load a MATLAB (.mat) file using scipy.io.loadmat():\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import the scipy.io package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype of mat\n",
    "print(type(mat))\n",
    "Explanation:\n",
    "Import scipy.io → This module provides functions for working with .mat files.\n",
    "Load the .mat file → scipy.io.loadmat('filename') reads the MATLAB file and stores the data as a Python dictionary.\n",
    "Print the datatype → type(mat) will likely return:\n",
    "javascript\n",
    "Copy\n",
    "Edit\n",
    "<class 'dict'>\n",
    "This means the .mat file contents are stored as a dictionary, where:\n",
    "Keys → Variable names from the MATLAB file.\n",
    "Values → Corresponding arrays or data structures.\n",
    "Next Steps:\n",
    "To check the file contents, print the dictionary keys:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "print(mat.keys())\n",
    "This will list the available variables inside albeck_gene_expression.mat.\n",
    "\n",
    "Would you like help extracting specific data from it? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "The structure of .mat in Python\n",
    "Here, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.\n",
    "\n",
    "The file 'albeck_gene_expression.mat' is already loaded into the variable mat. The following libraries have already been imported as follows:\n",
    "\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "Once again, this file contains gene expression data from the Albeck Lab at UCDavis.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
    "Print the type of the value corresponding to the key 'CYratioCyt' in mat. Recall that mat['CYratioCyt'] accesses the value.\n",
    "Print the shape of the value corresponding to the key 'CYratioCyt' using the numpy function shape().\n",
    "Execute the entire script to see some oscillatory gene expression data!\n",
    "\n",
    "# Print the keys of the MATLAB dictionary\n",
    "print(____)\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n",
    "ChatGPT said:\n",
    "Here's the correct Python script to analyze the structure of the .mat file and inspect the MATLAB dictionary:\n",
    "\n",
    "Solution:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]  # Extract a specific row and column range\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n",
    "Explanation:\n",
    "mat.keys() → Lists all variable names stored in the MATLAB .mat file.\n",
    "type(mat['CYratioCyt']) → Checks what kind of Python object CYratioCyt is (usually a NumPy array).\n",
    "np.shape(mat['CYratioCyt']) → Prints the dimensions (rows × columns) of CYratioCyt, which helps understand the dataset structure.\n",
    "Plotting gene expression data → Extracts data from row index 25, skipping the first 5 columns, then plots it.\n",
    "Expected Output:\n",
    "Console Output\n",
    "javascript\n",
    "Copy\n",
    "Edit\n",
    "dict_keys(['__header__', '__version__', '__globals__', 'CYratioCyt'])\n",
    "<class 'numpy.ndarray'>\n",
    "(67, 176)\n",
    "CYratioCyt is a NumPy array.\n",
    "It has 67 rows × 176 columns (example dimensions; yours may vary).\n",
    "Plot Output\n",
    "A time-series gene expression graph, showing fluorescence intensity over time.\n",
    "\n",
    "Would you like help in further analyzing or extracting specific data? 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "this file:\n",
    "\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "how would I view everything from the file\n",
    "ChatGPT said:\n",
    "To view everything in the .mat file (albeck_gene_expression.mat), you can follow these steps:\n",
    "\n",
    "1️⃣ Print All Keys (Variables in MATLAB File)\n",
    "This helps identify what data is stored in the file.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import scipy.io\n",
    "\n",
    "# Load the MATLAB file\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print all keys (variables inside the file)\n",
    "print(mat.keys())\n",
    "2️⃣ View Everything (Complete Dictionary)\n",
    "If you want to print everything, including metadata and raw data:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pprint  # Pretty-print for better readability\n",
    "\n",
    "# Print all content in a readable format\n",
    "pprint.pprint(mat)\n",
    "3️⃣ View Only MATLAB Data (Ignore Metadata)\n",
    "MATLAB .mat files often contain metadata like __header__, __version__, and __globals__. You can ignore those and focus on actual data:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Exclude metadata keys\n",
    "for key in mat:\n",
    "    if not key.startswith(\"__\"):\n",
    "        print(f\"Variable: {key}\")\n",
    "        print(mat[key])  # Print data\n",
    "        print(\"=\" * 50)  # Separator\n",
    "4️⃣ Check Data Types and Shapes\n",
    "To understand the structure of the stored variables:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import numpy as np\n",
    "\n",
    "# Loop through the variables and check their types/shapes\n",
    "for key in mat:\n",
    "    if not key.startswith(\"__\"):\n",
    "        print(f\"Variable: {key}\")\n",
    "        print(f\"Type: {type(mat[key])}\")\n",
    "        print(f\"Shape: {np.shape(mat[key])}\")\n",
    "        print(\"=\" * 50)\n",
    "5️⃣ If It Contains a NumPy Array (View Data as a DataFrame)\n",
    "If the data is structured like a table, converting it to a Pandas DataFrame makes it more readable:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "# Convert a variable (e.g., 'CYratioCyt') into a DataFrame\n",
    "df = pd.DataFrame(mat['CYratioCyt'])\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "6️⃣ Save the Data to CSV for Easier Exploration\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df.to_csv(\"gene_expression_data.csv\", index=False)\n",
    "print(\"Data saved to 'gene_expression_data.csv'\")\n",
    "Expected Output (Example)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "dict_keys(['__header__', '__version__', '__globals__', 'CYratioCyt'])\n",
    "\n",
    "Variable: CYratioCyt\n",
    "Type: <class 'numpy.ndarray'>\n",
    "Shape: (67, 176)\n",
    "==================================================\n",
    "Would you like to extract or visualize a specific dataset from the file? 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas to import flat files as DataFrames (2)\n",
    "In the last exercise, you were able to import flat files into a pandas DataFrame. As a bonus, it is then straightforward to retrieve the corresponding numpy array using the method .to_numpy(). You'll now have a chance to do this using the MNIST dataset, which is available as digits.csv.\n",
    "\n",
    "There are a number of arguments that pd.read_csv() takes that you'll find useful for this exercise:\n",
    "\n",
    "nrows allows you to specify how many rows to read from the file. For example, nrows=10 will only import the first 10 rows.\n",
    "header accepts row numbers to use as the column labels and marks the start of the data. If the file does not contain a header row, you can set header=None, and pandas will automatically assign integer column labels starting from 0 (e.g., 0, 1, 2, …).\n",
    "Instructions\n",
    "100 XP\n",
    "Import the first 5 rows of the file into a DataFrame using the function pd.read_csv() and assign the result to data. You'll need to use the arguments nrows and header. Note that there is no header row in this file.\n",
    "Build a numpy array from the resulting DataFrame in data and assign to data_array.\n",
    "Execute print(type(data_array)) to print the datatype of data_array.\n",
    "\n",
    "\n",
    "# Assign the filename: file\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame: data\n",
    "data = ____(____, ____, ____)\n",
    "\n",
    "# Build a numpy array from the DataFrame: data_array\n",
    "data_array = ____\n",
    "\n",
    "# Print the datatype of data_array to the shell\n",
    "print(type(data_array))\n",
    "ChatGPT said:\n",
    "Here's the complete solution using pandas to read the first 5 rows of digits.csv into a DataFrame and then converting it to a NumPy array:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the filename\n",
    "file = 'digits.csv'\n",
    "\n",
    "# Read the first 5 rows of the file into a DataFrame\n",
    "data = pd.read_csv(file, nrows=5, header=None)\n",
    "\n",
    "# Build a numpy array from the DataFrame\n",
    "data_array = data.to_numpy()\n",
    "\n",
    "# Print the datatype of data_array\n",
    "print(type(data_array))\n",
    "Explanation:\n",
    "pd.read_csv(file, nrows=5, header=None)\n",
    "Reads the first 5 rows from digits.csv (since nrows=5).\n",
    "Since there is no header row, header=None ensures pandas assigns default integer column names (0,1,2,...).\n",
    ".to_numpy()\n",
    "Converts the DataFrame into a NumPy array.\n",
    "print(type(data_array))\n",
    "Prints <class 'numpy.ndarray'> confirming that the DataFrame has been successfully converted.\n",
    "Let me know if you have any issues! 🚀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Customizing your pandas import\n",
    "The pandas package is great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values (NA or NaN). To wrap up this chapter, you're going to import a corrupted copy of the Titanic dataset titanic_corrupt.txt, which contains comments after the character '#', and is tab-delimited.\n",
    "\n",
    "Key arguments for pd.read_csv() include:\n",
    "\n",
    "sep sets the expected delimiter.\n",
    "You can use ',' for comma-delimited.\n",
    "You can use '\\t' for tab-delimited.\n",
    "comment takes characters that comments occur after in the file, indicating that any text starting with these characters should be ignored.\n",
    "na_values takes a list of strings to identify as NA/NaN. By default, some values are already recognized as NA/NaN. Providing this argument will supply additional values.\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the arguments of pd.read_csv() to import titanic_corrupt.txt correctly using pandas:\n",
    "sep sets the delimiter to use, and works the same way as np.loadtxt()'s delimiter argument. Note that the file you're importing is tab-delimited.\n",
    "comment takes characters that comments occur after in the file, which in this case is '#'.\n",
    "na_values takes a list of strings to be treated as NA/NaN, in this case the string 'Nothing'.\n",
    "Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the 'Age' of passengers aboard the Titanic.\n",
    "\n",
    "\n",
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='____', comment='____', na_values=[____])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "Reasoned for a second\n",
    "Below is the complete solution:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'titanic_corrupt.txt'\n",
    "\n",
    "# Import file: data\n",
    "data = pd.read_csv(file, sep='\\t', comment='#', na_values=['Nothing'])\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot 'Age' variable in a histogram\n",
    "pd.DataFrame.hist(data[['Age']])\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "Explanation:\n",
    "sep='\\t' tells pandas that the file is tab-delimited.\n",
    "comment='#' instructs pandas to ignore any text following # in the file.\n",
    "na_values=['Nothing'] treats the string \"Nothing\" as a missing value (NA/NaN).\n",
    "Feel free to ask any follow-up questions!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Customizing your spreadsheet import\n",
    "Here, you'll parse your spreadsheets and use additional arguments to skip rows, rename columns and select only particular columns.\n",
    "\n",
    "The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "As before, you'll use the method parse(). This time, however, you'll add the additional arguments skiprows, names and usecols. These skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists containing the specific row numbers, strings and column numbers, as appropriate.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Parse the first sheet by index. In doing so, skip the first row of data and name the columns 'Country' and 'AAM due to War (2002)' using the argument names. The values passed to skiprows and names all need to be of type list.\n",
    "Parse the second sheet by index. In doing so, parse only the first column with the usecols parameter, skip the first row and rename the column 'Country'. The argument passed to usecols also needs to be of type list.\n",
    "\n",
    "\n",
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(____, skiprows=____, names=____)\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(____, usecols=____, skiprows=____, names=____)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n",
    "Reasoned for a couple of seconds\n",
    "Below is the solution:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse(1, usecols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n",
    "Explanation:\n",
    "For df1, we:\n",
    "Parse the first sheet (index 0).\n",
    "Skip the first row with skiprows=[0].\n",
    "Rename columns to 'Country' and 'AAM due to War (2002)' using names=[...].\n",
    "For df2, we:\n",
    "Parse the second sheet (index 1).\n",
    "Use only the first column via usecols=[0].\n",
    "Skip the first row with skiprows=[0].\n",
    "Rename the column to 'Country' using names=['Country'].\n",
    "This approach customizes your import effectively for the required columns and rows.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "How to import SAS7BDAT\n",
    "How do you correctly import the function SAS7BDAT() from the package sas7bdat?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "import SAS7BDAT from sas7bdat\n",
    "Press\n",
    "1\n",
    "\n",
    "from SAS7BDAT import sas7bdat\n",
    "Press\n",
    "2\n",
    "\n",
    "import sas7bdat from SAS7BDAT\n",
    "Press\n",
    "3\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "Press\n",
    "4\n",
    "Reasoned for a second\n",
    "The correct way to import the SAS7BDAT function is:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sas7bdat import SAS7BDAT\n",
    "Thus, the correct answer is option 4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Importing SAS files\n",
    "In this exercise, you'll figure out how to import a SAS file as a DataFrame using SAS7BDAT and pandas. The file 'sales.sas7bdat' is already in your working directory and both pandas and matplotlib.pyplot have already been imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "The data are adapted from the website of the undergraduate text book Principles of Econometrics by Hill, Griffiths and Lim.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import the module SAS7BDAT from the library sas7bdat.\n",
    "In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method .to_data_frame() on the object file.\n",
    "Print the head of the DataFrame df_sas.\n",
    "Execute your entire script to produce a histogram plot!\n",
    "\n",
    "# Import sas7bdat package\n",
    "from ____ import ____\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    ____\n",
    "\n",
    "# Print head of DataFrame\n",
    "\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "Reasoned for a second\n",
    "Here's the complete solution:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "Explanation:\n",
    "Import Statement:\n",
    "We import SAS7BDAT from the sas7bdat package using:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sas7bdat import SAS7BDAT\n",
    "Loading the Data:\n",
    "Using a context manager (with), we open 'sales.sas7bdat' and then convert it into a DataFrame with:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df_sas = file.to_data_frame()\n",
    "Visualization:\n",
    "We plot a histogram of the column 'P' using pandas' DataFrame.hist() and matplotlib for additional plot customization.\n",
    "\n",
    "Run the script to see the DataFrame's head and the histogram plot!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Using read_stata to import Stata files\n",
    "The pandas package has been imported in the environment as pd and the file disarea.dta is in your working directory. The data consist of disease extents for several diseases in various countries (more information can be found here).\n",
    "\n",
    "What is the correct way of using the read_stata() function to import disarea.dta into the object df?\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "df = 'disarea.dta'\n",
    "\n",
    "df = read_stata.pd('disarea.dta')\n",
    "\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "df = pd.read_stata(disarea.dta)\n",
    "Reasoned for a second\n",
    "The correct way to import the Stata file is:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_stata('disarea.dta')\n",
    "Thus, the correct answer is the third option.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Importing Stata files\n",
    "Here, you'll gain expertise in importing Stata files as DataFrames using the pd.read_stata() function from pandas. The last exercise's file, 'disarea.dta', is still in your working directory.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use pd.read_stata() to load the file 'disarea.dta' into the DataFrame df.\n",
    "Print the head of the DataFrame df.\n",
    "Visualize your results by plotting a histogram of the column disa10. We’ve already provided this code for you, so just run it!\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()\n",
    "Reasoned for a couple of seconds\n",
    "Below is the complete solution:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()\n",
    "Explanation:\n",
    "pd.read_stata('disarea.dta') is used to load the Stata file into a DataFrame.\n",
    "print(df.head()) prints the first few rows of the DataFrame.\n",
    "A histogram of the column 'disa10' is plotted using DataFrame.hist().\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "how do I view all the content of this file using pure python code\n",
    "disarea.dta\n",
    "Reasoned for 4 seconds\n",
    "Stata .dta files are binary files, so \"viewing\" them in a human-readable format using pure Python (without specialized libraries) can be challenging. However, if you simply want to print the raw binary content, you can open the file in binary mode and read it:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open the file in binary mode and read its content\n",
    "with open('disarea.dta', 'rb') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Print the raw binary content (this may appear as gibberish)\n",
    "print(content)\n",
    "Note:\n",
    "\n",
    "The output will be a bytes object representing the raw binary data of the .dta file.\n",
    "If you want to interpret the content in a meaningful way (i.e., as a DataFrame), you should use a library like pandas (pd.read_stata('disarea.dta')), which is designed to parse and convert the binary format into a readable DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "I'm not expecting this\n",
    "\n",
    "b\"l\\x02\\x01\\x00\\x1b\\x00\\xa4\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00P\\xfb\\x12\\x00\\x05\\x05\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03\\x03=\\x1a\\xe7w\\xe0Un\\x00\\x0f\\x00\\x00\\x00\\x00\\x00\\x00\\x00N\\x1a\\xe7w\\x00\\x00\\x00\\x00\\xac\\x84X\\x00\\x00\\x00\\x00\\x00ZMB\\x00\\x00\\x00\\x00\\x00\\x91\\xb1T\\x00\\x9112 Jan 2000 11:45\\x00\\x82\\xa5fffffffffffffffffffffffffwbcode\\x00\\x00\\x00country\\x00\\x00disa1\\x00\\x00\\x00\\x00disa2\\x00\\x00\\x00\\x00disa3\\x00\\x00\\x00\\x00disa4\\x00\\x00\\x00\\x00disa5\\x00\\x00\\x00\\x00disa6\\x00\\x00\\x00\\x00disa7\\x00\\x00\\x00\\x00disa8\\x00\\x00\\x00\\x00disa9\\x00\\x00\\x00\\x00disa10\\x00\\x00\\x00disa11\\x00\\x00\\x00disa12\\x00\\x00\\x00disa13\\x00\\x00\\x00disa14\\x00\\x00\\x00disa15\\x00\\x00\\x00disa16\\x00\\x00\\x00disa17\\x00\\x00\\x00disa18\\x00\\x00\\x00disa19\\x00\\x00\\x00disa20\\x00\\x00\\x00disa21\\x00\\x00\\x00disa22\\x00\\x00\\x00disa23\\x00\\x00\\x00disa24\\x00\\x00\\x00disa25\\x00\\x00\n",
    "ChatGPT said:\n",
    "If you want to view the contents of the Stata .dta file in a readable format using pure Python (without pandas), you will need to parse the binary structure of the file. Stata's .dta format is complex, so manually parsing it is quite difficult. However, one approach is to use the struct module and decode the binary content step by step.\n",
    "\n",
    "Alternative: Using pyreadstat (Recommended)\n",
    "If you are okay with using an external library but still avoiding pandas, you can use pyreadstat to read .dta files.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pyreadstat\n",
    "\n",
    "# Read the Stata file\n",
    "df, meta = pyreadstat.read_dta(\"disarea.dta\")\n",
    "\n",
    "# Print the data (df is a dictionary-like object)\n",
    "print(df)\n",
    "This will output the file content in a structured format.\n",
    "\n",
    "Pure Python Approach (Without External Libraries)\n",
    "Stata .dta files store:\n",
    "\n",
    "A header with metadata\n",
    "Column names\n",
    "Data values in binary format\n",
    "You can extract column names and data using basic Python, but it's not a trivial task. Below is a simple attempt to extract readable parts from the binary file:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Open the file in binary mode\n",
    "with open(\"disarea.dta\", \"rb\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Attempt to decode readable text from the binary file\n",
    "readable_content = content.decode(errors=\"ignore\")\n",
    "\n",
    "# Print a portion of the readable content\n",
    "print(readable_content[:1000])  # Print first 1000 characters to avoid too much output\n",
    "This will show:\n",
    "\n",
    "Column names\n",
    "Some metadata\n",
    "Possibly some readable values\n",
    "However, the best approach remains using pyreadstat or pandas.read_stata() since they handle all the parsing complexities for you.\n",
    "\n",
    "Would you like a deeper breakdown of Stata's binary format, or is using pyreadstat an option for you?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
